{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\n\n\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\").to(device)","metadata":{"execution":{"iopub.status.busy":"2024-11-09T12:04:48.956800Z","iopub.execute_input":"2024-11-09T12:04:48.957595Z","iopub.status.idle":"2024-11-09T12:06:44.988825Z","shell.execute_reply.started":"2024-11-09T12:04:48.957552Z","shell.execute_reply":"2024-11-09T12:06:44.987880Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"135fce0c97fa4e14a2e0ed245f5f4445"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:  66%|######6   | 3.28G/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32bc850afbb84677824e5ba914ced713"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"206ab3129e994bed9e8399065d6e215b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b018db7d085c41c1b20cf7edfffb4330"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/195 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3049840e6064de197a1125f9cc01f88"}},"metadata":{}}]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom datasets import DatasetDict\nimport torch\n\ndataset = load_dataset(\"keivalya/MedQuad-MedicalQnADataset\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntrain_test_split = dataset['train'].train_test_split(test_size=0.2)\n\ndataset = DatasetDict({\n    'train': train_test_split['train'],\n    'test': train_test_split['test']\n})\n\ndataset\n","metadata":{"execution":{"iopub.status.busy":"2024-11-09T12:06:44.990405Z","iopub.execute_input":"2024-11-09T12:06:44.990761Z","iopub.status.idle":"2024-11-09T12:06:47.582965Z","shell.execute_reply.started":"2024-11-09T12:06:44.990726Z","shell.execute_reply":"2024-11-09T12:06:47.582005Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['qtype', 'Question', 'Answer'],\n        num_rows: 13125\n    })\n    test: Dataset({\n        features: ['qtype', 'Question', 'Answer'],\n        num_rows: 3282\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"def prepare_data_for_inference(examples):\n    inputs = [f\"Q: {q}\\nThink like a medical professional step by step.\" for q in examples['Question']]\n    return inputs\n\ntest_questions = prepare_data_for_inference(dataset['test'])","metadata":{"execution":{"iopub.status.busy":"2024-11-09T12:06:47.583925Z","iopub.execute_input":"2024-11-09T12:06:47.584202Z","iopub.status.idle":"2024-11-09T12:06:47.619043Z","shell.execute_reply.started":"2024-11-09T12:06:47.584172Z","shell.execute_reply":"2024-11-09T12:06:47.618371Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", message=\"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\")","metadata":{"execution":{"iopub.status.busy":"2024-11-09T12:06:47.620772Z","iopub.execute_input":"2024-11-09T12:06:47.621083Z","iopub.status.idle":"2024-11-09T12:06:47.626070Z","shell.execute_reply.started":"2024-11-09T12:06:47.621051Z","shell.execute_reply":"2024-11-09T12:06:47.625188Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n\ndef generate_baseline_predictions(model, tokenizer, questions):\n    model.eval()  # Set the model to evaluation mode\n    predictions = []\n    \n    for question in tqdm(questions, desc=\"Generating predictions\"):\n        inputs = tokenizer(f\"{question}\", return_tensors=\"pt\").to(device)\n        outputs = model.generate(inputs['input_ids'], max_length=70)\n        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        predictions.append(response)\n    \n    return predictions\n\npredictions = generate_baseline_predictions(model, tokenizer, test_questions)","metadata":{"execution":{"iopub.status.busy":"2024-11-09T12:07:51.345137Z","iopub.execute_input":"2024-11-09T12:07:51.345855Z","iopub.status.idle":"2024-11-09T15:27:36.815312Z","shell.execute_reply.started":"2024-11-09T12:07:51.345812Z","shell.execute_reply":"2024-11-09T15:27:36.814365Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"Generating predictions:   0%|          | 0/3282 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nStarting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\nGenerating predictions: 100%|██████████| 3282/3282 [3:19:45<00:00,  3.65s/it]  \n","output_type":"stream"}]},{"cell_type":"code","source":"!pip -q install evaluate\n!pip -q install rouge-score\n\nimport evaluate\nreferences = dataset['test']['Answer']","metadata":{"execution":{"iopub.status.busy":"2024-11-09T15:27:41.192472Z","iopub.execute_input":"2024-11-09T15:27:41.193392Z","iopub.status.idle":"2024-11-09T15:28:22.268092Z","shell.execute_reply.started":"2024-11-09T15:27:41.193341Z","shell.execute_reply":"2024-11-09T15:28:22.267010Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"rouge = evaluate.load(\"rouge\")\nscores = rouge.compute(predictions=predictions, references=references)\nprint(scores)","metadata":{"execution":{"iopub.status.busy":"2024-11-09T15:28:35.213195Z","iopub.execute_input":"2024-11-09T15:28:35.213961Z","iopub.status.idle":"2024-11-09T15:29:13.950133Z","shell.execute_reply.started":"2024-11-09T15:28:35.213918Z","shell.execute_reply":"2024-11-09T15:29:13.949053Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1042c535d5ce42c4b10f9134bf69e461"}},"metadata":{}},{"name":"stdout","text":"{'rouge1': 0.17937144193498764, 'rouge2': 0.06156423640657316, 'rougeL': 0.13169716671422033, 'rougeLsum': 0.13355506567934036}\n","output_type":"stream"}]},{"cell_type":"code","source":"bleu = evaluate.load(\"bleu\")\nscores = bleu.compute(predictions=predictions, references=references)\nprint(scores)","metadata":{"execution":{"iopub.status.busy":"2024-11-09T15:30:22.895937Z","iopub.execute_input":"2024-11-09T15:30:22.896435Z","iopub.status.idle":"2024-11-09T15:30:34.649990Z","shell.execute_reply.started":"2024-11-09T15:30:22.896398Z","shell.execute_reply":"2024-11-09T15:30:34.648841Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a2e639c8b4445e19dc0c00d4d49cbcb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca3645ffa7dc4f37b5adb554f7cf8b3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec2522cc740542da843227aab6269a8e"}},"metadata":{}},{"name":"stdout","text":"{'bleu': 0.0021513336194622468, 'precisions': [0.3986281880596236, 0.1183092579995465, 0.06006245568101238, 0.033595569876274566], 'brevity_penalty': 0.021781601379091, 'length_ratio': 0.20718133437175493, 'translation_length': 153228, 'reference_length': 739584}\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.DataFrame({\n    'predictions': predictions,\n    'references': references\n})\n\n\ndf.to_csv('/kaggle/working/predictions_references.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-11-09T15:35:00.591214Z","iopub.execute_input":"2024-11-09T15:35:00.592316Z","iopub.status.idle":"2024-11-09T15:35:00.805446Z","shell.execute_reply.started":"2024-11-09T15:35:00.592257Z","shell.execute_reply":"2024-11-09T15:35:00.804561Z"},"trusted":true},"execution_count":12,"outputs":[]}]}