{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(token=\"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\")\n",
        "pip install datasets\n",
        "pip install together"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEH8KVeZxAXd",
        "outputId": "47857492-3d6d-4ad9-f996-b066b9cca3e9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from together import Together\n",
        "import re\n",
        "\n",
        "# Configure API Key and Client\n",
        "os.environ[\"TOGETHER_API_KEY\"] = \"b0b71b939e051a6ef95d2e24e434bb7867b42d80aec84d5f4155ae6a829caa2b\"\n",
        "client = Together(api_key=os.environ[\"TOGETHER_API_KEY\"])\n",
        "\n",
        "def infer(question, choices):\n",
        "    options_text = \" \".join([f\"[Option {chr(idx + 65)}] {choice}\" for idx, choice in enumerate(choices)])\n",
        "    prompt = f\"Question: {question} {options_text} Which option is correct?\"\n",
        "\n",
        "    stream = client.chat.completions.create(\n",
        "        model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        stream=True,\n",
        "        max_tokens=500\n",
        "    )\n",
        "\n",
        "    response = \"\"\n",
        "    for chunk in stream:\n",
        "        response += chunk.choices[0].delta.content or \"\"\n",
        "\n",
        "    lines = response.strip().split('\\n')\n",
        "    first_line = lines[0] if lines else \"\"\n",
        "    last_line = lines[-1] if lines else \"\"\n",
        "\n",
        "    match = re.search(r'Option ([A-D])', first_line)\n",
        "    if not match:\n",
        "        match = re.search(r'Option ([A-D])', last_line)\n",
        "        if not match:\n",
        "            match = re.search(r'Option ([A-D])|\\b([A-D])\\b', response)\n",
        "\n",
        "    if match:\n",
        "        predicted_answer = match.group(1) if match.group(1) else match.group(2)\n",
        "    else:\n",
        "        predicted_answer = \"No clear answer found\"\n",
        "\n",
        "    return predicted_answer\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"GBaker/MedQA-USMLE-4-options\", split='test')\n",
        "\n",
        "correct_predictions = 0\n",
        "total_predictions = 0\n",
        "test_limit = 1273\n",
        "\n",
        "for i, sample in enumerate(tqdm(dataset)):\n",
        "    if i >= test_limit:\n",
        "        break\n",
        "    question = sample['question']\n",
        "    options = sample['options'].values()\n",
        "    correct_answer = sample['answer_idx']\n",
        "\n",
        "    predicted_answer = infer(question, list(options))\n",
        "\n",
        "    if predicted_answer == correct_answer:\n",
        "        correct_predictions += 1\n",
        "    total_predictions += 1\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = (correct_predictions / total_predictions) * 100\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7DKrKEneu7W",
        "outputId": "70510822-d376-4a4e-94c2-130ee10e25ee"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1273/1273 [36:06<00:00,  1.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 52.87%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}