{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30787,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bb4bbd3192c84c25845e331057a210f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_07be6799e6e34bbda98e7ba8b8161a87",
              "IPY_MODEL_f80a6a25e3ff4834855970ed8f857fb3",
              "IPY_MODEL_3708cb8f982643cfb759fd7e02667866"
            ],
            "layout": "IPY_MODEL_05753c4615ce4c028d7bb105b4298963"
          }
        },
        "07be6799e6e34bbda98e7ba8b8161a87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe278cc74de84e4bbb0029802011264b",
            "placeholder": "​",
            "style": "IPY_MODEL_76423858e3254d2e87a01591fa700db5",
            "value": "adapter_config.json: 100%"
          }
        },
        "f80a6a25e3ff4834855970ed8f857fb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1bd0e6a74d641a5b79759014b348c68",
            "max": 768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1d155ef4b586499aac655ad31abb16f0",
            "value": 768
          }
        },
        "3708cb8f982643cfb759fd7e02667866": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_695421e76b6d4563a5624535778d2078",
            "placeholder": "​",
            "style": "IPY_MODEL_9336b2366fd249e9a4ceb07bbfd98705",
            "value": " 768/768 [00:00&lt;00:00, 32.1kB/s]"
          }
        },
        "05753c4615ce4c028d7bb105b4298963": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe278cc74de84e4bbb0029802011264b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76423858e3254d2e87a01591fa700db5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a1bd0e6a74d641a5b79759014b348c68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d155ef4b586499aac655ad31abb16f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "695421e76b6d4563a5624535778d2078": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9336b2366fd249e9a4ceb07bbfd98705": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "027ce99a37de42a0bf76a91eecf29ed2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7976429f7369444ebfcc0e0579cf3cb8",
              "IPY_MODEL_c85a3ea9757e4439870bde5ec4b1f2e5",
              "IPY_MODEL_754f6077c82e4cd3a0dafa514e6f9f37"
            ],
            "layout": "IPY_MODEL_e9196b23fb924ab3b08ef7df5315e4e9"
          }
        },
        "7976429f7369444ebfcc0e0579cf3cb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e9f858253b544f3b100f702873b8d45",
            "placeholder": "​",
            "style": "IPY_MODEL_cb41a5034ff04dea89d2ff75aca9e4ce",
            "value": "config.json: 100%"
          }
        },
        "c85a3ea9757e4439870bde5ec4b1f2e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3616e92aa2af4a6d8f117444feb758d6",
            "max": 855,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8d9bfe7ae7a341419ec44bf31d0e2000",
            "value": 855
          }
        },
        "754f6077c82e4cd3a0dafa514e6f9f37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5148881185db4294a43cf1fa27296d33",
            "placeholder": "​",
            "style": "IPY_MODEL_142312e0269f45a086558ba3fc69dc13",
            "value": " 855/855 [00:00&lt;00:00, 47.4kB/s]"
          }
        },
        "e9196b23fb924ab3b08ef7df5315e4e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e9f858253b544f3b100f702873b8d45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb41a5034ff04dea89d2ff75aca9e4ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3616e92aa2af4a6d8f117444feb758d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d9bfe7ae7a341419ec44bf31d0e2000": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5148881185db4294a43cf1fa27296d33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "142312e0269f45a086558ba3fc69dc13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba673d6169644abc94cde721fc380b26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d4208ab6765e462dba15496ea9f59233",
              "IPY_MODEL_c21848be409b4f9889ceb25d46e9045a",
              "IPY_MODEL_81b98dcecec14f5a84b7f6b9f08ba0cb"
            ],
            "layout": "IPY_MODEL_da02e9a4a0a5414abfcf34d56f026d93"
          }
        },
        "d4208ab6765e462dba15496ea9f59233": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9461be8a0fba4f6498f95189c1765183",
            "placeholder": "​",
            "style": "IPY_MODEL_02127b4adbf749b4870e6579f5b020a2",
            "value": "model.safetensors.index.json: 100%"
          }
        },
        "c21848be409b4f9889ceb25d46e9045a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f5b34b689da48dda6da98e9fa62340b",
            "max": 23950,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e07720d16f4143fb894240cb4b422542",
            "value": 23950
          }
        },
        "81b98dcecec14f5a84b7f6b9f08ba0cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_371c1d6d228c4803be7fe593fd4e699d",
            "placeholder": "​",
            "style": "IPY_MODEL_cc294ab7822f48da893fd51bfa8395bb",
            "value": " 23.9k/23.9k [00:00&lt;00:00, 900kB/s]"
          }
        },
        "da02e9a4a0a5414abfcf34d56f026d93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9461be8a0fba4f6498f95189c1765183": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02127b4adbf749b4870e6579f5b020a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f5b34b689da48dda6da98e9fa62340b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e07720d16f4143fb894240cb4b422542": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "371c1d6d228c4803be7fe593fd4e699d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc294ab7822f48da893fd51bfa8395bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d574665011c4d3084de12dc41be2da5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_19ca59c32a344283889434ca1535af55",
              "IPY_MODEL_1e5541c31bdb4231bf66de136d2337e6",
              "IPY_MODEL_3649a43675a04ec0ab5c68246bee8f37"
            ],
            "layout": "IPY_MODEL_08030eb5ba9f4b29bc4525b064e581d7"
          }
        },
        "19ca59c32a344283889434ca1535af55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3fe83e6d8d444c8a8a5aceb183b7389e",
            "placeholder": "​",
            "style": "IPY_MODEL_fa93f25cdbc3471d8d096365c585bd21",
            "value": "Downloading shards: 100%"
          }
        },
        "1e5541c31bdb4231bf66de136d2337e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_057538ed65dd4a4492db76fcaa08ab5b",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a95ab0a971ab489fab6ca739f51dc884",
            "value": 4
          }
        },
        "3649a43675a04ec0ab5c68246bee8f37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ffd41f35095843209dae08a01434c368",
            "placeholder": "​",
            "style": "IPY_MODEL_acd85d4245244ac8a58a254f2afe9c6a",
            "value": " 4/4 [06:30&lt;00:00, 84.21s/it]"
          }
        },
        "08030eb5ba9f4b29bc4525b064e581d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3fe83e6d8d444c8a8a5aceb183b7389e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa93f25cdbc3471d8d096365c585bd21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "057538ed65dd4a4492db76fcaa08ab5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a95ab0a971ab489fab6ca739f51dc884": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ffd41f35095843209dae08a01434c368": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acd85d4245244ac8a58a254f2afe9c6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7bcc1e2eb90b453a9be703ebc2d97343": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9f287c249015441c8d12802af0641fbb",
              "IPY_MODEL_c68f090656f2469e9b9498ef0d689334",
              "IPY_MODEL_2bb599710f774115a7a65135af38b66e"
            ],
            "layout": "IPY_MODEL_e73bc93d5a4542a6b3e51144980d058c"
          }
        },
        "9f287c249015441c8d12802af0641fbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39a6f47ac3c74b03a8e9cbf1f0125765",
            "placeholder": "​",
            "style": "IPY_MODEL_c9094c8c4d75439e900a90db887d886f",
            "value": "model-00001-of-00004.safetensors: 100%"
          }
        },
        "c68f090656f2469e9b9498ef0d689334": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_862c48f14cab423fa8a8160890de39de",
            "max": 4976698672,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1dfaec271cf14b93bf58ec378a37d778",
            "value": 4976698672
          }
        },
        "2bb599710f774115a7a65135af38b66e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f351936f0d52438daa6aec31f833a23a",
            "placeholder": "​",
            "style": "IPY_MODEL_51911f3ab22049dd99fb7ed26120f040",
            "value": " 4.98G/4.98G [02:01&lt;00:00, 42.2MB/s]"
          }
        },
        "e73bc93d5a4542a6b3e51144980d058c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39a6f47ac3c74b03a8e9cbf1f0125765": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9094c8c4d75439e900a90db887d886f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "862c48f14cab423fa8a8160890de39de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1dfaec271cf14b93bf58ec378a37d778": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f351936f0d52438daa6aec31f833a23a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51911f3ab22049dd99fb7ed26120f040": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3594790a791e4547b607e4925fee4edf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d11b67f5a7ec45ab867491a15a47bd23",
              "IPY_MODEL_f8513dbb1dd343cd80a111a67d7bf2c7",
              "IPY_MODEL_b3d4db02ec1d4d48be66422615eee3c1"
            ],
            "layout": "IPY_MODEL_18fa0483209d4158910397adea8c45bd"
          }
        },
        "d11b67f5a7ec45ab867491a15a47bd23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6778480f71d14e6fb17399980dc0da97",
            "placeholder": "​",
            "style": "IPY_MODEL_274753366eb9439a80ba945e6b3586bb",
            "value": "model-00002-of-00004.safetensors: 100%"
          }
        },
        "f8513dbb1dd343cd80a111a67d7bf2c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6b7030ed48f449b8a4724491979da74",
            "max": 4999802720,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_641db1ae9f924ac798ee775c9c63c24c",
            "value": 4999802720
          }
        },
        "b3d4db02ec1d4d48be66422615eee3c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c9f0d29c671423194fb26f6b72dbd81",
            "placeholder": "​",
            "style": "IPY_MODEL_24a2c072d525412da29f263f17bfa375",
            "value": " 5.00G/5.00G [01:59&lt;00:00, 41.6MB/s]"
          }
        },
        "18fa0483209d4158910397adea8c45bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6778480f71d14e6fb17399980dc0da97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "274753366eb9439a80ba945e6b3586bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f6b7030ed48f449b8a4724491979da74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "641db1ae9f924ac798ee775c9c63c24c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9c9f0d29c671423194fb26f6b72dbd81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24a2c072d525412da29f263f17bfa375": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c0f360b4323e44e1a33644b47c5efd1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bcadfb805e4747b1b97c367ef337290b",
              "IPY_MODEL_cf43d2a1b5d043a79f85fd21499a3094",
              "IPY_MODEL_ec51851b5c8d4db49b301a075eeba4f3"
            ],
            "layout": "IPY_MODEL_4b2bd485c4af4b8eacefacb2760793dd"
          }
        },
        "bcadfb805e4747b1b97c367ef337290b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14fe9ed2df1d40778f4ebb869e6ce2c3",
            "placeholder": "​",
            "style": "IPY_MODEL_4a0b10d2ac1443ffb04bec92b8852abc",
            "value": "model-00003-of-00004.safetensors: 100%"
          }
        },
        "cf43d2a1b5d043a79f85fd21499a3094": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6c8d04bd7024f7d997431b82856cf6a",
            "max": 4915916176,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_770ac17a361d41329947082dabc2ed72",
            "value": 4915916176
          }
        },
        "ec51851b5c8d4db49b301a075eeba4f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21799559dc5a4ac9816bfc85067252ac",
            "placeholder": "​",
            "style": "IPY_MODEL_526fff2cb5324ec8a8ab039e1be5b865",
            "value": " 4.92G/4.92G [02:00&lt;00:00, 42.0MB/s]"
          }
        },
        "4b2bd485c4af4b8eacefacb2760793dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14fe9ed2df1d40778f4ebb869e6ce2c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a0b10d2ac1443ffb04bec92b8852abc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b6c8d04bd7024f7d997431b82856cf6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "770ac17a361d41329947082dabc2ed72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "21799559dc5a4ac9816bfc85067252ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "526fff2cb5324ec8a8ab039e1be5b865": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "78e118e2feaf4e289611e8629aefea42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_95faa6a8c5b84067b8383fb5faa370e1",
              "IPY_MODEL_7c21075bc39048d19e612e0816c6793d",
              "IPY_MODEL_a433be6fc5734d82896d036e1f102479"
            ],
            "layout": "IPY_MODEL_93e03d101c4b479fa516fbcb6aaee59c"
          }
        },
        "95faa6a8c5b84067b8383fb5faa370e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a49cb5fb1cc546f789960d54fac8a9ed",
            "placeholder": "​",
            "style": "IPY_MODEL_37e542150ba943cb9894eaa4549a3f69",
            "value": "model-00004-of-00004.safetensors: 100%"
          }
        },
        "7c21075bc39048d19e612e0816c6793d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa1c1c0f55eb49fcbc57bce707a30c8e",
            "max": 1168138808,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7cd79c85d6b1499b8470ad3ad83b92e5",
            "value": 1168138808
          }
        },
        "a433be6fc5734d82896d036e1f102479": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_812510b0e421456792b865607cc09dd6",
            "placeholder": "​",
            "style": "IPY_MODEL_926f08ab8278466ba5106761273b5050",
            "value": " 1.17G/1.17G [00:27&lt;00:00, 41.2MB/s]"
          }
        },
        "93e03d101c4b479fa516fbcb6aaee59c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a49cb5fb1cc546f789960d54fac8a9ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37e542150ba943cb9894eaa4549a3f69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa1c1c0f55eb49fcbc57bce707a30c8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cd79c85d6b1499b8470ad3ad83b92e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "812510b0e421456792b865607cc09dd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "926f08ab8278466ba5106761273b5050": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e3007a5d645941f39b713e65799a408b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3613c80ee0294bac909983fbcef6a372",
              "IPY_MODEL_b20b4a58d438481d87c4b19a7b915547",
              "IPY_MODEL_9cbf69105df44c539d973aecf7f01b04"
            ],
            "layout": "IPY_MODEL_729efbaf45a14428b4ea312c51d478f9"
          }
        },
        "3613c80ee0294bac909983fbcef6a372": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9474f1a631bd4e4f878d818110ea17bc",
            "placeholder": "​",
            "style": "IPY_MODEL_db0f1f70acf9454a9c4d74c9ad128408",
            "value": "Loading checkpoint shards:  25%"
          }
        },
        "b20b4a58d438481d87c4b19a7b915547": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1e3b94b9ef645b8a891b358fd746dd9",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_31d35fed31104649b6b533ff1c9fa882",
            "value": 1
          }
        },
        "9cbf69105df44c539d973aecf7f01b04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0cb320a643d046898d9f0e68f1ae5fbd",
            "placeholder": "​",
            "style": "IPY_MODEL_6beaa1bbec9c4337a74187b9a7bf4106",
            "value": " 1/4 [00:23&lt;01:11, 23.78s/it]"
          }
        },
        "729efbaf45a14428b4ea312c51d478f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9474f1a631bd4e4f878d818110ea17bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db0f1f70acf9454a9c4d74c9ad128408": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d1e3b94b9ef645b8a891b358fd746dd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31d35fed31104649b6b533ff1c9fa882": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0cb320a643d046898d9f0e68f1ae5fbd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6beaa1bbec9c4337a74187b9a7bf4106": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e170f241a2e24b3b9730f404ee1d80ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_91a1cf7cb6354cf8910d59a42370443b",
              "IPY_MODEL_e31de9ba843b443bb87ee14be68a2938",
              "IPY_MODEL_bf3cea062e5949fe82f9bdb4f0c355b8"
            ],
            "layout": "IPY_MODEL_e7466e57f5004c578f3c6fdb04698861"
          }
        },
        "91a1cf7cb6354cf8910d59a42370443b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90f618a232614be98106e0fccfa91d8d",
            "placeholder": "​",
            "style": "IPY_MODEL_67842758c5d345ce8a2283e5ed928919",
            "value": "Loading checkpoint shards:  50%"
          }
        },
        "e31de9ba843b443bb87ee14be68a2938": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_575a9aef814f465599a21e1afb68e19f",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4cb03b9e952449e09e887894c151c78b",
            "value": 2
          }
        },
        "bf3cea062e5949fe82f9bdb4f0c355b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6f5abf26cfa4e29a1d0ccaef4ba4a07",
            "placeholder": "​",
            "style": "IPY_MODEL_3c0023edd2a74b19b082a361e0c1812a",
            "value": " 2/4 [00:45&lt;00:45, 22.95s/it]"
          }
        },
        "e7466e57f5004c578f3c6fdb04698861": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90f618a232614be98106e0fccfa91d8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67842758c5d345ce8a2283e5ed928919": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "575a9aef814f465599a21e1afb68e19f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cb03b9e952449e09e887894c151c78b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c6f5abf26cfa4e29a1d0ccaef4ba4a07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c0023edd2a74b19b082a361e0c1812a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "114fc82f96424289935b7d9ad804e32c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_98ce82ea2e18476c89a853302fb5dd71",
              "IPY_MODEL_cfe4ea58953a44edb8d490424505f7e0",
              "IPY_MODEL_e057bf5ccf1e4e929669c2264b0db57a"
            ],
            "layout": "IPY_MODEL_9d47c73eeb51417f8297c2be5b366535"
          }
        },
        "98ce82ea2e18476c89a853302fb5dd71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd0e86679f5d467fa965847a712f3139",
            "placeholder": "​",
            "style": "IPY_MODEL_c5f302e9dd54482db5997d8e9a32631d",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "cfe4ea58953a44edb8d490424505f7e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca7b6749979e4b16960d48e9b47bf48b",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ead2090afdbb46cc924942f608a51733",
            "value": 4
          }
        },
        "e057bf5ccf1e4e929669c2264b0db57a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7556196966854524b39f0a1df2ac3430",
            "placeholder": "​",
            "style": "IPY_MODEL_f91efa3ca6f4408e956785b3f8125414",
            "value": " 4/4 [01:22&lt;00:00, 18.32s/it]"
          }
        },
        "9d47c73eeb51417f8297c2be5b366535": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd0e86679f5d467fa965847a712f3139": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5f302e9dd54482db5997d8e9a32631d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca7b6749979e4b16960d48e9b47bf48b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ead2090afdbb46cc924942f608a51733": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7556196966854524b39f0a1df2ac3430": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f91efa3ca6f4408e956785b3f8125414": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "646f0ef629ba452081ecc97efe0fe137": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0fc8ec620ea7497986c1f9b0512b6a0f",
              "IPY_MODEL_68b29981bfe44749b542c4eacf08d5eb",
              "IPY_MODEL_6fecec50372441e9a2f91911905251f6"
            ],
            "layout": "IPY_MODEL_f9c48257923241d7898032d45167d2b8"
          }
        },
        "0fc8ec620ea7497986c1f9b0512b6a0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39fd9bcced31413a9b72d226844ebd13",
            "placeholder": "​",
            "style": "IPY_MODEL_9b6183ffe3644dbc932965a0c9ff1516",
            "value": "generation_config.json: 100%"
          }
        },
        "68b29981bfe44749b542c4eacf08d5eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_faa2ee602796441b9158cecceaabe64f",
            "max": 184,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f7ae0088b97c4d2eacacdf1b8663ad82",
            "value": 184
          }
        },
        "6fecec50372441e9a2f91911905251f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39d3e36ef78c487998c81275d6d4d8a0",
            "placeholder": "​",
            "style": "IPY_MODEL_31e713c992484547b6025baa5f0b94c4",
            "value": " 184/184 [00:00&lt;00:00, 5.36kB/s]"
          }
        },
        "f9c48257923241d7898032d45167d2b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39fd9bcced31413a9b72d226844ebd13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b6183ffe3644dbc932965a0c9ff1516": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "faa2ee602796441b9158cecceaabe64f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7ae0088b97c4d2eacacdf1b8663ad82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "39d3e36ef78c487998c81275d6d4d8a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31e713c992484547b6025baa5f0b94c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "39c1b8b4c53e44e484cd7f7e873c98a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2e286e5faf2f4eba96a0c06e98db8c4f",
              "IPY_MODEL_8eb0eb8e68c5442fb046b35529a6b652",
              "IPY_MODEL_7ff7427e38a141d48ae4d8a958e1765c"
            ],
            "layout": "IPY_MODEL_ab60028b192346fa878b268bdfba47d4"
          }
        },
        "2e286e5faf2f4eba96a0c06e98db8c4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1025ae8b0728485ca4dc1de9d0828899",
            "placeholder": "​",
            "style": "IPY_MODEL_54a4a4dda4dc4f15811f0588f588c798",
            "value": "adapter_model.safetensors: 100%"
          }
        },
        "8eb0eb8e68c5442fb046b35529a6b652": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eaedd96341564798a1a25a00ac32cf9b",
            "max": 2453893592,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_41d0ecef14024e98920fff2c1bd7878a",
            "value": 2453893592
          }
        },
        "7ff7427e38a141d48ae4d8a958e1765c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_221f2e7b9b834d1aa2b44b167252748d",
            "placeholder": "​",
            "style": "IPY_MODEL_c4200656dd08414883ffdfeedd00eb45",
            "value": " 2.45G/2.45G [01:00&lt;00:00, 42.2MB/s]"
          }
        },
        "ab60028b192346fa878b268bdfba47d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1025ae8b0728485ca4dc1de9d0828899": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54a4a4dda4dc4f15811f0588f588c798": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eaedd96341564798a1a25a00ac32cf9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41d0ecef14024e98920fff2c1bd7878a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "221f2e7b9b834d1aa2b44b167252748d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4200656dd08414883ffdfeedd00eb45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a213757d51249fa95cef80175a30c0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_277bbcaa8710466eadabc51966c01227",
              "IPY_MODEL_c2c5f1f06ae04c839af15bbb286e926d",
              "IPY_MODEL_3ad2139ea4fb4393a8dc14ed7081edbf"
            ],
            "layout": "IPY_MODEL_ee6eb05a97524faba51c457dc60d2626"
          }
        },
        "277bbcaa8710466eadabc51966c01227": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4ba92138644472fbf20f065e029a262",
            "placeholder": "​",
            "style": "IPY_MODEL_b23148b8806543d9bc593c778bba11a1",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "c2c5f1f06ae04c839af15bbb286e926d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6cfaee70391e4b1983d82e6e3a1ef768",
            "max": 55380,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0affc9dbb6294436b482957694c1b62c",
            "value": 55380
          }
        },
        "3ad2139ea4fb4393a8dc14ed7081edbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_437dc0ec3a9c44cfb9b1952104335ef7",
            "placeholder": "​",
            "style": "IPY_MODEL_ee8e2eb7dd52428999873ef57bd26a3d",
            "value": " 55.4k/55.4k [00:00&lt;00:00, 1.41MB/s]"
          }
        },
        "ee6eb05a97524faba51c457dc60d2626": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4ba92138644472fbf20f065e029a262": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b23148b8806543d9bc593c778bba11a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6cfaee70391e4b1983d82e6e3a1ef768": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0affc9dbb6294436b482957694c1b62c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "437dc0ec3a9c44cfb9b1952104335ef7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee8e2eb7dd52428999873ef57bd26a3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e0a1023a344b45868c1de253ce50ebb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f51c77c2d3e545cea993f0095225ceb0",
              "IPY_MODEL_3f614f2b83344d5e9d33106078dfac5c",
              "IPY_MODEL_97cf7b4da9784879ad4872933445bcc9"
            ],
            "layout": "IPY_MODEL_962147aa23854b0ab7910b04eaf36e51"
          }
        },
        "f51c77c2d3e545cea993f0095225ceb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b638394d8d6e498fb6a9b813405625bb",
            "placeholder": "​",
            "style": "IPY_MODEL_b6a3dc84a7094eb8ba92fd9b7322af55",
            "value": "tokenizer.json: 100%"
          }
        },
        "3f614f2b83344d5e9d33106078dfac5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8eef344b95db460f8b4e0d84f35d0064",
            "max": 17210195,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1af5fc3fcc9a4efe83eb3c7f16cb2a71",
            "value": 17210195
          }
        },
        "97cf7b4da9784879ad4872933445bcc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0424b3f3d20439081bf7a7cf484b774",
            "placeholder": "​",
            "style": "IPY_MODEL_b238a3ea65444706b10d7b94cac7591e",
            "value": " 17.2M/17.2M [00:00&lt;00:00, 40.4MB/s]"
          }
        },
        "962147aa23854b0ab7910b04eaf36e51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b638394d8d6e498fb6a9b813405625bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6a3dc84a7094eb8ba92fd9b7322af55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8eef344b95db460f8b4e0d84f35d0064": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1af5fc3fcc9a4efe83eb3c7f16cb2a71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c0424b3f3d20439081bf7a7cf484b774": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b238a3ea65444706b10d7b94cac7591e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43db7c9194054331b4191f1fd5992bc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e080237748c246f3bec88bb2cdacba46",
              "IPY_MODEL_b62c77da20b4468ab1e5ff9bfd6b22e4",
              "IPY_MODEL_30e6cb117b714f9ea120071ca896af2e"
            ],
            "layout": "IPY_MODEL_61e2f66f2659484e89d266adcbac53c2"
          }
        },
        "e080237748c246f3bec88bb2cdacba46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3955223c44824d889930e7e255fc2fd9",
            "placeholder": "​",
            "style": "IPY_MODEL_8e33a6b8b22a4130b656c299f988e28f",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "b62c77da20b4468ab1e5ff9bfd6b22e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b0c8da98f1c490a960ba8520734e095",
            "max": 325,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_08e8d081b3bd4edbb89b3e86ba5e6d56",
            "value": 325
          }
        },
        "30e6cb117b714f9ea120071ca896af2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d7b2b2413fa4c9b8e7244df76f4d3e9",
            "placeholder": "​",
            "style": "IPY_MODEL_9a9038b418844797970d8a3b9c0238ce",
            "value": " 325/325 [00:00&lt;00:00, 21.8kB/s]"
          }
        },
        "61e2f66f2659484e89d266adcbac53c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3955223c44824d889930e7e255fc2fd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e33a6b8b22a4130b656c299f988e28f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b0c8da98f1c490a960ba8520734e095": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08e8d081b3bd4edbb89b3e86ba5e6d56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2d7b2b2413fa4c9b8e7244df76f4d3e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a9038b418844797970d8a3b9c0238ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-11-17T15:00:52.050277Z",
          "iopub.execute_input": "2024-11-17T15:00:52.050660Z",
          "iopub.status.idle": "2024-11-17T15:01:16.278986Z",
          "shell.execute_reply.started": "2024-11-17T15:00:52.050596Z",
          "shell.execute_reply": "2024-11-17T15:01:16.277988Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDLNNf7uZtMl",
        "outputId": "7101bd69-0825-43af-b717-854a197d1c8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.3.1)\n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch\n",
        "\n",
        "!pip install -q -U accelerate peft bitsandbytes transformers trl einops"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-17T15:01:16.280411Z",
          "iopub.execute_input": "2024-11-17T15:01:16.280746Z",
          "iopub.status.idle": "2024-11-17T15:01:42.467233Z",
          "shell.execute_reply.started": "2024-11-17T15:01:16.280696Z",
          "shell.execute_reply": "2024-11-17T15:01:42.466026Z"
        },
        "trusted": true,
        "id": "Zm0GvscHZtMo"
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T15:01:42.469422Z",
          "iopub.execute_input": "2024-11-17T15:01:42.469759Z",
          "iopub.status.idle": "2024-11-17T15:01:42.474497Z",
          "shell.execute_reply.started": "2024-11-17T15:01:42.469724Z",
          "shell.execute_reply": "2024-11-17T15:01:42.473471Z"
        },
        "id": "VCh52hAi6gXZ"
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "80FpsAq4ZtMp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, PeftModel\n",
        "\n",
        "from transformers import (\n",
        "\n",
        "    AutoModelForCausalLM,\n",
        "\n",
        "    AutoTokenizer,\n",
        "\n",
        "    BitsAndBytesConfig,\n",
        "\n",
        "    TrainingArguments,\n",
        "\n",
        "    logging,\n",
        "\n",
        ")\n",
        "\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-17T15:01:42.475606Z",
          "iopub.execute_input": "2024-11-17T15:01:42.475865Z",
          "iopub.status.idle": "2024-11-17T15:02:00.839922Z",
          "shell.execute_reply.started": "2024-11-17T15:01:42.475823Z",
          "shell.execute_reply": "2024-11-17T15:02:00.839115Z"
        },
        "trusted": true,
        "id": "63iUXpN3ZtMq"
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"GBaker/MedQA-USMLE-4-options\")\n",
        "\n",
        "train_samples = dataset['train']\n",
        "\n",
        "test_samples = dataset['test']\n",
        "\n",
        "train_samples = train_samples.shuffle(seed=42).select(range(3500))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_validation_split = train_samples.train_test_split(test_size=0.2, seed=42)\n",
        "\n",
        "train_samples = train_validation_split['train']\n",
        "\n",
        "val_samples = train_validation_split['test']\n",
        "\n",
        "val_samples = val_samples.select(range(100))\n",
        "\n",
        "\n",
        "\n",
        "print(len(train_samples))\n",
        "\n",
        "print(len(val_samples))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-17T15:02:00.841993Z",
          "iopub.execute_input": "2024-11-17T15:02:00.842624Z",
          "iopub.status.idle": "2024-11-17T15:02:02.989318Z",
          "shell.execute_reply.started": "2024-11-17T15:02:00.842578Z",
          "shell.execute_reply": "2024-11-17T15:02:02.988364Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLCp3Mp2ZtMq",
        "outputId": "67da939f-7393-40e4-b2cb-22be75aad1f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2800\n",
            "100\n"
          ]
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prompt(example):\n",
        "\n",
        "    idx_mapping = {'A': 1, 'B': 2, 'C': 3, 'D': 4}\n",
        "\n",
        "\n",
        "\n",
        "    question, options, answer_idx = example['question'], example['options'], example['answer_idx']\n",
        "\n",
        "\n",
        "\n",
        "    answer = idx_mapping[answer_idx]\n",
        "\n",
        "\n",
        "\n",
        "    option_list = [f\"{value}\" for key, value in options.items()]\n",
        "\n",
        "\n",
        "\n",
        "    return (\n",
        "\n",
        "        f\"Answer the following question by returning one correct numerical value:\\n\"\n",
        "\n",
        "        f\"Question : {question}\\n\"\n",
        "\n",
        "        f\"Option 1: {option_list[0]}\\n\"\n",
        "\n",
        "        f\"Option 2: {option_list[1]}\\n\"\n",
        "\n",
        "        f\"Option 3: {option_list[2]}\\n\"\n",
        "\n",
        "        f\"Option 4: {option_list[3]}\\n\"\n",
        "\n",
        "        f\"Answer only one numerical value. \\n\"\n",
        "\n",
        "        f\"Answer:\"\n",
        "\n",
        "    )"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-17T15:02:11.781731Z",
          "iopub.execute_input": "2024-11-17T15:02:11.782614Z",
          "iopub.status.idle": "2024-11-17T15:02:11.788981Z",
          "shell.execute_reply.started": "2024-11-17T15:02:11.782571Z",
          "shell.execute_reply": "2024-11-17T15:02:11.787851Z"
        },
        "trusted": true,
        "id": "1a13kTZXZtMr"
      },
      "outputs": [],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "source": [
        "answer_mapping = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4}\n",
        "\n",
        "\n",
        "\n",
        "def generate_prompt_and_label(sample):\n",
        "\n",
        "    prompt = generate_prompt(sample)\n",
        "\n",
        "\n",
        "\n",
        "    label = answer_mapping.get(sample['answer_idx'], 0)\n",
        "\n",
        "\n",
        "\n",
        "    label_tensor = torch.tensor(label)\n",
        "\n",
        "\n",
        "\n",
        "    return {\"text\": prompt, \"labels\": label_tensor}\n",
        "\n",
        "\n",
        "\n",
        "train_samples = train_samples.map(generate_prompt_and_label)\n",
        "\n",
        "val_samples = val_samples.map(generate_prompt_and_label)\n",
        "\n",
        "test_samples = test_samples.map(generate_prompt_and_label)\n",
        "\n",
        "print(train_samples[0])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-17T15:02:14.668974Z",
          "iopub.execute_input": "2024-11-17T15:02:14.669419Z",
          "iopub.status.idle": "2024-11-17T15:02:15.701668Z",
          "shell.execute_reply.started": "2024-11-17T15:02:14.669375Z",
          "shell.execute_reply": "2024-11-17T15:02:15.700752Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHI27Wb2ZtMs",
        "outputId": "21574d89-4661-44a7-fba7-90732a8beeed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'question': 'A 60-year-old man comes to the physician because of flank pain, rash, and blood-tinged urine for 1 day. Two months ago, he was started on hydrochlorothiazide for hypertension. He takes acetaminophen for back pain. Examination shows a generalized, diffuse maculopapular rash. Serum studies show a creatinine concentration of 3.0 mg/dL. Renal ultrasonography shows no abnormalities. Which of the following findings is most likely to be observed in this patient?', 'answer': 'Urinary eosinophils', 'options': {'A': 'Dermal IgA deposition on skin biopsy', 'B': 'Crescent-shape extracapillary cell proliferation', 'C': 'Mesangial IgA deposits on renal biopsy', 'D': 'Urinary eosinophils'}, 'meta_info': 'step1', 'answer_idx': 'D', 'metamap_phrases': ['60 year old man', 'physician', 'flank pain', 'rash', 'blood urine', '1 day', 'Two months', 'started', 'hydrochlorothiazide', 'hypertension', 'takes acetaminophen', 'back pain', 'Examination shows', 'generalized', 'diffuse maculopapular rash', 'Serum studies show', 'creatinine concentration', '3.0 mg/dL', 'Renal ultrasonography shows', 'abnormalities', 'following findings', 'most likely to', 'observed', 'patient'], 'text': 'Answer the following question by returning one correct numerical value:\\nQuestion : A 60-year-old man comes to the physician because of flank pain, rash, and blood-tinged urine for 1 day. Two months ago, he was started on hydrochlorothiazide for hypertension. He takes acetaminophen for back pain. Examination shows a generalized, diffuse maculopapular rash. Serum studies show a creatinine concentration of 3.0 mg/dL. Renal ultrasonography shows no abnormalities. Which of the following findings is most likely to be observed in this patient?\\nOption 1: Dermal IgA deposition on skin biopsy\\nOption 2: Crescent-shape extracapillary cell proliferation\\nOption 3: Mesangial IgA deposits on renal biopsy\\nOption 4: Urinary eosinophils\\nAnswer only one numerical value. \\nAnswer:', 'labels': 4}\n"
          ]
        }
      ],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "\n",
        "\n",
        "login(token=\"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\")"
      ],
      "metadata": {
        "id": "UTC1vkayadF-",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T15:02:18.622671Z",
          "iopub.execute_input": "2024-11-17T15:02:18.623290Z",
          "iopub.status.idle": "2024-11-17T15:02:18.679555Z",
          "shell.execute_reply.started": "2024-11-17T15:02:18.623251Z",
          "shell.execute_reply": "2024-11-17T15:02:18.678641Z"
        }
      },
      "outputs": [],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "d33bfa864a614f279da1fdcdc0f9c683",
            "19b1369caf344caf8deae3025779916a",
            "65e6ef4b0187468a8bc616b3280f2c4e"
          ]
        },
        "id": "HZUv-hyjaTlo",
        "outputId": "5050e03a-ac54-43ce-cb7d-8d79c0e06d8e",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T14:23:10.605225Z",
          "iopub.execute_input": "2024-11-17T14:23:10.605598Z",
          "iopub.status.idle": "2024-11-17T14:23:21.619668Z",
          "shell.execute_reply.started": "2024-11-17T14:23:10.605556Z",
          "shell.execute_reply": "2024-11-17T14:23:21.618563Z"
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d33bfa864a614f279da1fdcdc0f9c683"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "19b1369caf344caf8deae3025779916a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "65e6ef4b0187468a8bc616b3280f2c4e"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Train dataset: {'input_ids': tensor([[128000,  16533,    279,  ...,    301,   1073,    581],\n        [128000,  16533,    279,  ...,     25,  10836,    398],\n        [128000,  16533,    279,  ..., 128009, 128009, 128009],\n        ...,\n        [128000,  16533,    279,  ..., 128009, 128009, 128009],\n        [128000,  16533,    279,  ...,     19,     25,  19111],\n        [128000,  16533,    279,  ..., 128009, 128009, 128009]],\n       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 0, 0, 0],\n        ...,\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([2, 1, 1,  ..., 2, 2, 1], device='cuda:0')}\nValidation dataset: {'input_ids': tensor([[128000,  16533,    279,  ..., 128009, 128009, 128009],\n        [128000,  16533,    279,  ...,   1097,  15959,  61597],\n        [128000,  16533,    279,  ..., 128009, 128009, 128009],\n        ...,\n        [128000,  16533,    279,  ..., 128009, 128009, 128009],\n        [128000,  16533,    279,  ..., 128009, 128009, 128009],\n        [128000,  16533,    279,  ..., 128009, 128009, 128009]],\n       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 0, 0, 0],\n        ...,\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1, 3, 4, 2, 1, 2, 4, 4, 3, 4, 2, 2, 2, 4, 2, 2, 4, 1, 4, 2, 3, 4, 1, 1,\n        2, 2, 2, 2, 1, 2, 1, 1, 2, 3, 4, 1, 1, 2, 2, 3, 2, 4, 3, 4, 2, 4, 4, 1,\n        4, 2, 4, 3, 3, 1, 3, 1, 3, 4, 4, 3, 3, 3, 1, 1, 3, 1, 4, 4, 3, 3, 1, 3,\n        4, 2, 3, 1, 3, 4, 4, 1, 1, 1, 1, 3, 2, 3, 1, 1, 2, 2, 2, 2, 4, 1, 3, 3,\n        2, 1, 1, 4], device='cuda:0')}\nTest dataset: {'input_ids': tensor([[128000,  16533,    279,  ..., 128009, 128009, 128009],\n        [128000,  16533,    279,  ..., 128009, 128009, 128009],\n        [128000,  16533,    279,  ...,    402,  50804,  99291],\n        ...,\n        [128000,  16533,    279,  ...,     19,     25,  78196],\n        [128000,  16533,    279,  ..., 128009, 128009, 128009],\n        [128000,  16533,    279,  ..., 128009, 128009, 128009]],\n       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 1, 1, 1],\n        ...,\n        [1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([2, 4, 2,  ..., 2, 3, 3], device='cuda:0')}\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset['input_ids'][0])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-17T14:23:21.623705Z",
          "iopub.execute_input": "2024-11-17T14:23:21.624063Z",
          "iopub.status.idle": "2024-11-17T14:23:21.633784Z",
          "shell.execute_reply.started": "2024-11-17T14:23:21.624030Z",
          "shell.execute_reply": "2024-11-17T14:23:21.632887Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGwk7OJJZtMt",
        "outputId": "13f19ea4-1e67-4019-fbe6-b7bdcbfe2e90"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "tensor([128000,  16533,    279,   2768,   3488,    555,  13758,    832,   4495,\n         35876,    907,    512,  14924,    551,    362,    220,   4331,   4771,\n          6418,    893,    374,   1694,  26126,    369,    264,    220,     18,\n         30609,   3925,    315,  36709,     11,  17250,    311,  37455,     11,\n         22709,     79,  33252,    449,  43844,    290,     11,    294,  97745,\n            11,    323,   7528,   6784,    430,  36050,    449,   9439,     13,\n          1283,    706,  48637,   4376,    264,   3854,    315,  34576,    264,\n          1938,   2533,    568,    574,    220,    508,     13,   5414,   1510,\n          6593,   3925,  18065,  63308,     13,   1283,   5097,    665,    278,\n           391,  31660,   7446,     13,    578,  16595,  12195,   2997,    264,\n          6680,   7410,    315,    220,   9263,     14,   6086,   9653,    473,\n            70,     11,    264,   4851,   4478,    315,    220,   2721,  45273,\n            11,    323,    264,   9499,    315,    220,   1927,     13,     24,\n         32037,    320,   3264,     13,     19,  59572,    570,   1952,   7106,\n         24481,     11,  12786,    268,    638,     70,   5893,    374,   1766,\n            13,    362,   4686,   6680,   1797,  21667,  84664,     65,    511,\n         16820,  10934,    315,    220,   7007,     11,    931,   7917,   3262,\n            18,     13,  11868,    990,   4726,   5039,  25983,  41529,  11245,\n            11,  11245,  50843,     11,    323,  41529,  58139,  85986,    323,\n          7319,   2860,  11245,  11212,   8824,     13,    362,   6680,  81603,\n         21667,    459,   7319,   1396,    315,  35663,  12235,  10145,     11,\n           323,    264,  17685,  83748,  25689,    349,  11007,    279,   9546,\n           315,  22709,    501,   5174,  19262,    587,    661,  57878,     13,\n           362,  27472,    389,    813,  51815,    220,     24,  43496,    279,\n         28378,    753,  38141,    315,    264,   3738,   1206,  25180,    856,\n        104701,   1098,  11691,   1413,   8624,     13,    578,   8893,    374,\n          3940,    389,  17055,   4223,    554,     64,     13,   3639,    374,\n           279,   1455,   4461,  23842,   5380,   5454,    220,     16,     25,\n          3092,    301,   1073,    581], device='cuda:0')\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "\n",
        "\n",
        "# Specify the LLaMA model\n",
        "\n",
        "base_model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "\n",
        "\n",
        "\n",
        "# Load the tokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Set EOS token as the padding token\n",
        "\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define tokenization function\n",
        "\n",
        "def tokenize_function(prompts, labels):\n",
        "\n",
        "    encodings = tokenizer(prompts, padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\")\n",
        "\n",
        "    encodings['labels'] = torch.tensor(labels, dtype=torch.long)  # Ensure labels are in the correct type\n",
        "\n",
        "    return encodings\n",
        "\n",
        "\n",
        "\n",
        "# Assuming train_samples, val_samples, and test_samples are lists of dictionaries\n",
        "\n",
        "# with \"text\" and \"labels\" keys\n",
        "\n",
        "train_prompts = [example['text'] for example in train_samples]\n",
        "\n",
        "train_labels = [example['labels'] for example in train_samples]\n",
        "\n",
        "val_prompts = [example['text'] for example in val_samples]\n",
        "\n",
        "val_labels = [example['labels'] for example in val_samples]\n",
        "\n",
        "test_prompts = [example['text'] for example in test_samples]\n",
        "\n",
        "test_labels = [example['labels'] for example in test_samples]\n",
        "\n",
        "\n",
        "\n",
        "# Tokenize the datasets\n",
        "\n",
        "train_dataset = tokenize_function(train_prompts, train_labels)\n",
        "\n",
        "val_dataset = tokenize_function(val_prompts, val_labels)\n",
        "\n",
        "test_dataset = tokenize_function(test_prompts, test_labels)\n",
        "\n",
        "\n",
        "\n",
        "# Move datasets to the correct device (if necessary)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_dataset['input_ids'] = train_dataset['input_ids'].to(device)\n",
        "\n",
        "train_dataset['labels'] = train_dataset['labels'].to(device)\n",
        "\n",
        "val_dataset['input_ids'] = val_dataset['input_ids'].to(device)\n",
        "\n",
        "val_dataset['labels'] = val_dataset['labels'].to(device)\n",
        "\n",
        "test_dataset['input_ids'] = test_dataset['input_ids'].to(device)\n",
        "\n",
        "test_dataset['labels'] = test_dataset['labels'].to(device)\n",
        "\n",
        "\n",
        "\n",
        "# Preview the tokenized dataset\n",
        "\n",
        "print(f\"Train dataset: {train_dataset}\")\n",
        "\n",
        "print(f\"Validation dataset: {val_dataset}\")\n",
        "\n",
        "print(f\"Test dataset: {test_dataset}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsGOMk5la0t1",
        "outputId": "5868ef3c-a59c-471d-8004-c784dcb12629",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T15:02:25.446396Z",
          "iopub.execute_input": "2024-11-17T15:02:25.446815Z",
          "iopub.status.idle": "2024-11-17T15:09:59.659886Z",
          "shell.execute_reply.started": "2024-11-17T15:02:25.446776Z",
          "shell.execute_reply": "2024-11-17T15:09:59.658891Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset: {'input_ids': tensor([[128000,  16533,    279,  ..., 128009, 128009, 128009],\n",
            "        [128000,  16533,    279,  ..., 128009, 128009, 128009],\n",
            "        [128000,  16533,    279,  ..., 128009, 128009, 128009],\n",
            "        ...,\n",
            "        [128000,  16533,    279,  ..., 128009, 128009, 128009],\n",
            "        [128000,  16533,    279,  ..., 128009, 128009, 128009],\n",
            "        [128000,  16533,    279,  ..., 128009, 128009, 128009]],\n",
            "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([4, 4, 1,  ..., 2, 2, 1], device='cuda:0')}\n",
            "Validation dataset: {'input_ids': tensor([[128000,  16533,    279,  ..., 128009, 128009, 128009],\n",
            "        [128000,  16533,    279,  ..., 128009, 128009, 128009],\n",
            "        [128000,  16533,    279,  ...,     16,     25,  16165],\n",
            "        ...,\n",
            "        [128000,  16533,    279,  ..., 128009, 128009, 128009],\n",
            "        [128000,  16533,    279,  ...,  87067,    555,    666],\n",
            "        [128000,  16533,    279,  ..., 128009, 128009, 128009]],\n",
            "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([4, 1, 3, 1, 1, 4, 1, 1, 3, 3, 1, 2, 1, 2, 3, 3, 3, 2, 1, 1, 3, 2, 2, 3,\n",
            "        3, 2, 2, 2, 4, 1, 3, 2, 3, 2, 3, 3, 2, 4, 2, 3, 4, 2, 1, 3, 4, 2, 2, 2,\n",
            "        3, 2, 2, 4, 1, 2, 4, 1, 1, 1, 1, 4, 4, 4, 3, 1, 4, 4, 2, 3, 3, 3, 1, 4,\n",
            "        3, 4, 1, 1, 4, 4, 2, 4, 4, 3, 2, 2, 3, 1, 1, 3, 2, 2, 1, 3, 1, 2, 2, 2,\n",
            "        2, 4, 1, 3], device='cuda:0')}\n",
            "Test dataset: {'input_ids': tensor([[128000,  16533,    279,  ..., 128009, 128009, 128009],\n",
            "        [128000,  16533,    279,  ..., 128009, 128009, 128009],\n",
            "        [128000,  16533,    279,  ...,    402,  50804,  99291],\n",
            "        ...,\n",
            "        [128000,  16533,    279,  ...,     19,     25,  78196],\n",
            "        [128000,  16533,    279,  ..., 128009, 128009, 128009],\n",
            "        [128000,  16533,    279,  ..., 128009, 128009, 128009]],\n",
            "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([2, 4, 2,  ..., 2, 3, 3], device='cuda:0')}\n"
          ]
        }
      ],
      "execution_count": 10
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xSf2NIU0ZtMv",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.use_cache = False\n",
        "\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-17T15:09:59.857952Z",
          "iopub.execute_input": "2024-11-17T15:09:59.858327Z",
          "iopub.status.idle": "2024-11-17T15:09:59.885211Z",
          "shell.execute_reply.started": "2024-11-17T15:09:59.858283Z",
          "shell.execute_reply": "2024-11-17T15:09:59.884221Z"
        },
        "trusted": true,
        "id": "KQQVGoL0ZtMv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, encodings):\n",
        "\n",
        "        self.encodings = encodings\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "\n",
        "        return item\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.encodings['input_ids'])\n",
        "\n",
        "\n",
        "\n",
        "# Create datasets\n",
        "\n",
        "train_dataset_custom = CustomDataset(train_dataset)\n",
        "\n",
        "val_dataset_custom = CustomDataset(val_dataset)\n",
        "\n",
        "test_dataset_custom = CustomDataset(test_dataset)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-17T15:09:59.887481Z",
          "iopub.execute_input": "2024-11-17T15:09:59.887767Z",
          "iopub.status.idle": "2024-11-17T15:09:59.894537Z",
          "shell.execute_reply.started": "2024-11-17T15:09:59.887736Z",
          "shell.execute_reply": "2024-11-17T15:09:59.893599Z"
        },
        "trusted": true,
        "id": "4yH8Zo_cZtMw"
      },
      "outputs": [],
      "execution_count": 11
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(\n",
        "\n",
        "    r=32,                   # default=8, higher value for more representational capacity\n",
        "\n",
        "    lora_alpha=64,\n",
        "\n",
        "    lora_dropout=0.05,\n",
        "\n",
        "    bias=\"none\",\n",
        "\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "\n",
        "    target_modules=[\n",
        "\n",
        "        \"q_proj\",\n",
        "\n",
        "        \"k_proj\",\n",
        "\n",
        "        \"v_proj\",\n",
        "\n",
        "        \"o_proj\",\n",
        "\n",
        "        \"gate_proj\",\n",
        "\n",
        "        \"up_proj\",\n",
        "\n",
        "        \"down_proj\",\n",
        "\n",
        "        \"lm_head\",\n",
        "\n",
        "        \"dense\"\n",
        "\n",
        "    ]\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "model = PeftModel(model, peft_config)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-17T15:09:59.895624Z",
          "iopub.execute_input": "2024-11-17T15:09:59.895904Z",
          "iopub.status.idle": "2024-11-17T15:10:01.159171Z",
          "shell.execute_reply.started": "2024-11-17T15:09:59.895874Z",
          "shell.execute_reply": "2024-11-17T15:10:01.158111Z"
        },
        "trusted": true,
        "id": "8BYctQIEZtMw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "training_arguments = TrainingArguments(\n",
        "\n",
        "    output_dir=\"./results\",\n",
        "\n",
        "    num_train_epochs=1,\n",
        "\n",
        "    fp16=False,\n",
        "\n",
        "    bf16=True,\n",
        "\n",
        "    per_device_train_batch_size=4,\n",
        "\n",
        "    per_device_eval_batch_size=4,\n",
        "\n",
        "    gradient_accumulation_steps=1,\n",
        "\n",
        "    gradient_checkpointing=True,\n",
        "\n",
        "    max_grad_norm=0.3,\n",
        "\n",
        "    learning_rate=2e-5,\n",
        "\n",
        "    weight_decay=0.001,\n",
        "\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "\n",
        "    max_steps=-1,\n",
        "\n",
        "    warmup_ratio=0.03,\n",
        "\n",
        "    group_by_length=True,\n",
        "\n",
        "    logging_strategy=\"epoch\",\n",
        "\n",
        "    evaluation_strategy=\"epoch\",  # Evaluate during training at each eval_step\n",
        "\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-17T15:10:01.160642Z",
          "iopub.execute_input": "2024-11-17T15:10:01.160992Z",
          "iopub.status.idle": "2024-11-17T15:10:01.205208Z",
          "shell.execute_reply.started": "2024-11-17T15:10:01.160955Z",
          "shell.execute_reply": "2024-11-17T15:10:01.204113Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIBWD9gUZtMx",
        "outputId": "84af86da-7528-4175-df97-ac5bb35846c5"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "\n",
        "    model=model,\n",
        "\n",
        "    train_dataset=train_dataset_custom,\n",
        "\n",
        "    eval_dataset=val_dataset_custom,\n",
        "\n",
        "    peft_config=peft_config,\n",
        "\n",
        "    tokenizer=tokenizer,\n",
        "\n",
        "    max_seq_length= 200,\n",
        "\n",
        "    args=training_arguments,\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "trainer.can_return_loss=True"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-17T15:10:01.206421Z",
          "iopub.execute_input": "2024-11-17T15:10:01.206726Z",
          "iopub.status.idle": "2024-11-17T15:10:02.034893Z",
          "shell.execute_reply.started": "2024-11-17T15:10:01.206691Z",
          "shell.execute_reply": "2024-11-17T15:10:02.033906Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFm8nA5lZtMx",
        "outputId": "953543b0-c882-42ac-e8f7-d419ff645bfb"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '0.13.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# 782ed76bbbf87938f52295cbc422c82c984e15af\n",
        "\n",
        "import wandb\n",
        "\n",
        "# Login using your WandB API key\n",
        "wandb.login(key=\"782ed76bbbf87938f52295cbc422c82c984e15af\")\n"
      ],
      "metadata": {
        "id": "QXIbKJoVcGXA",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T15:10:02.038402Z",
          "iopub.execute_input": "2024-11-17T15:10:02.038742Z",
          "iopub.status.idle": "2024-11-17T15:10:03.125592Z",
          "shell.execute_reply.started": "2024-11-17T15:10:02.038707Z",
          "shell.execute_reply": "2024-11-17T15:10:03.124587Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa148ca2-faee-4b45-e2f1-9e2599af1d7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-17T15:10:03.126759Z",
          "iopub.execute_input": "2024-11-17T15:10:03.127335Z",
          "iopub.status.idle": "2024-11-17T20:23:16.051996Z",
          "shell.execute_reply.started": "2024-11-17T15:10:03.127300Z",
          "shell.execute_reply": "2024-11-17T20:23:16.051113Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287,
          "referenced_widgets": [
            "31b0ca69f9d3471382538bc6326503c1"
          ]
        },
        "id": "14ZtWlJUZtMy",
        "outputId": "44aa6c95-f9c4-4db2-910e-56089f22af83"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlakshya21535\u001b[0m (\u001b[33mnlp_assgn\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113083255557083, max=1.0…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "31b0ca69f9d3471382538bc6326503c1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.18.3"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20241117_151003-ek1tjmhs</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/nlp_assgn/huggingface/runs/ek1tjmhs' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/nlp_assgn/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/nlp_assgn/huggingface' target=\"_blank\">https://wandb.ai/nlp_assgn/huggingface</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/nlp_assgn/huggingface/runs/ek1tjmhs' target=\"_blank\">https://wandb.ai/nlp_assgn/huggingface/runs/ek1tjmhs</a>"
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='700' max='700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [700/700 5:12:45, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.230100</td>\n      <td>1.204668</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:227: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:227: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
          "output_type": "stream"
        },
        {
          "execution_count": 17,
          "output_type": "execute_result",
          "data": {
            "text/plain": "TrainOutput(global_step=700, training_loss=1.2301105608258929, metrics={'train_runtime': 18792.4134, 'train_samples_per_second': 0.149, 'train_steps_per_second': 0.037, 'total_flos': 3.26561723056128e+16, 'train_loss': 1.2301105608258929, 'epoch': 1.0})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# print(f\"Model is running on: {next(model.parameters()).device}\")\n",
        "\n",
        "# trainer.save_model(\"./llama_finetuned_mcq/final_model\")\n",
        "\n",
        "\n",
        "trainer.push_to_hub(\"llama_finetuned_mcq\")\n",
        "\n",
        "# from transformers import AutoModelForCausalLM\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"your_username/finetuned_mcq\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "f1a5e68ae3c243f88b9d4c1bfbf34265",
            "abd97b54b74c4381a43a963e77369ecb",
            "9a0737351fb444f58741a69c7ed07d6f",
            "3f7697fd97e44517b3d6b0e3ac2d867b",
            "8d3f7e5d0f2247f7b9935c9bf9a09996"
          ]
        },
        "id": "PcEZ5GlGdest",
        "outputId": "219871d9-e65b-4337-d118-f04eca6ffd7c",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T20:23:59.187667Z",
          "iopub.execute_input": "2024-11-17T20:23:59.188076Z",
          "iopub.status.idle": "2024-11-17T20:25:13.758823Z",
          "shell.execute_reply.started": "2024-11-17T20:23:59.188023Z",
          "shell.execute_reply": "2024-11-17T20:25:13.757866Z"
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f1a5e68ae3c243f88b9d4c1bfbf34265"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "adapter_model.safetensors:   0%|          | 0.00/2.45G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "abd97b54b74c4381a43a963e77369ecb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "events.out.tfevents.1731856203.3820981303c4.30.0:   0%|          | 0.00/6.95k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a0737351fb444f58741a69c7ed07d6f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Upload 4 LFS files:   0%|          | 0/4 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3f7697fd97e44517b3d6b0e3ac2d867b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "training_args.bin:   0%|          | 0.00/5.56k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8d3f7e5d0f2247f7b9935c9bf9a09996"
            }
          },
          "metadata": {}
        },
        {
          "execution_count": 19,
          "output_type": "execute_result",
          "data": {
            "text/plain": "CommitInfo(commit_url='https://huggingface.co/Lakshya388/results/commit/9318ae30c5fed6671b012a825e168012cacc872e', commit_message='llama_finetuned_mcq', commit_description='', oid='9318ae30c5fed6671b012a825e168012cacc872e', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Lakshya388/results', endpoint='https://huggingface.co', repo_type='model', repo_id='Lakshya388/results'), pr_revision=None, pr_num=None)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Lakshya388/results\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T20:30:15.882724Z",
          "iopub.execute_input": "2024-11-17T20:30:15.883124Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305,
          "referenced_widgets": [
            "bb4bbd3192c84c25845e331057a210f0",
            "07be6799e6e34bbda98e7ba8b8161a87",
            "f80a6a25e3ff4834855970ed8f857fb3",
            "3708cb8f982643cfb759fd7e02667866",
            "05753c4615ce4c028d7bb105b4298963",
            "fe278cc74de84e4bbb0029802011264b",
            "76423858e3254d2e87a01591fa700db5",
            "a1bd0e6a74d641a5b79759014b348c68",
            "1d155ef4b586499aac655ad31abb16f0",
            "695421e76b6d4563a5624535778d2078",
            "9336b2366fd249e9a4ceb07bbfd98705",
            "027ce99a37de42a0bf76a91eecf29ed2",
            "7976429f7369444ebfcc0e0579cf3cb8",
            "c85a3ea9757e4439870bde5ec4b1f2e5",
            "754f6077c82e4cd3a0dafa514e6f9f37",
            "e9196b23fb924ab3b08ef7df5315e4e9",
            "5e9f858253b544f3b100f702873b8d45",
            "cb41a5034ff04dea89d2ff75aca9e4ce",
            "3616e92aa2af4a6d8f117444feb758d6",
            "8d9bfe7ae7a341419ec44bf31d0e2000",
            "5148881185db4294a43cf1fa27296d33",
            "142312e0269f45a086558ba3fc69dc13",
            "ba673d6169644abc94cde721fc380b26",
            "d4208ab6765e462dba15496ea9f59233",
            "c21848be409b4f9889ceb25d46e9045a",
            "81b98dcecec14f5a84b7f6b9f08ba0cb",
            "da02e9a4a0a5414abfcf34d56f026d93",
            "9461be8a0fba4f6498f95189c1765183",
            "02127b4adbf749b4870e6579f5b020a2",
            "1f5b34b689da48dda6da98e9fa62340b",
            "e07720d16f4143fb894240cb4b422542",
            "371c1d6d228c4803be7fe593fd4e699d",
            "cc294ab7822f48da893fd51bfa8395bb",
            "5d574665011c4d3084de12dc41be2da5",
            "19ca59c32a344283889434ca1535af55",
            "1e5541c31bdb4231bf66de136d2337e6",
            "3649a43675a04ec0ab5c68246bee8f37",
            "08030eb5ba9f4b29bc4525b064e581d7",
            "3fe83e6d8d444c8a8a5aceb183b7389e",
            "fa93f25cdbc3471d8d096365c585bd21",
            "057538ed65dd4a4492db76fcaa08ab5b",
            "a95ab0a971ab489fab6ca739f51dc884",
            "ffd41f35095843209dae08a01434c368",
            "acd85d4245244ac8a58a254f2afe9c6a",
            "7bcc1e2eb90b453a9be703ebc2d97343",
            "9f287c249015441c8d12802af0641fbb",
            "c68f090656f2469e9b9498ef0d689334",
            "2bb599710f774115a7a65135af38b66e",
            "e73bc93d5a4542a6b3e51144980d058c",
            "39a6f47ac3c74b03a8e9cbf1f0125765",
            "c9094c8c4d75439e900a90db887d886f",
            "862c48f14cab423fa8a8160890de39de",
            "1dfaec271cf14b93bf58ec378a37d778",
            "f351936f0d52438daa6aec31f833a23a",
            "51911f3ab22049dd99fb7ed26120f040",
            "3594790a791e4547b607e4925fee4edf",
            "d11b67f5a7ec45ab867491a15a47bd23",
            "f8513dbb1dd343cd80a111a67d7bf2c7",
            "b3d4db02ec1d4d48be66422615eee3c1",
            "18fa0483209d4158910397adea8c45bd",
            "6778480f71d14e6fb17399980dc0da97",
            "274753366eb9439a80ba945e6b3586bb",
            "f6b7030ed48f449b8a4724491979da74",
            "641db1ae9f924ac798ee775c9c63c24c",
            "9c9f0d29c671423194fb26f6b72dbd81",
            "24a2c072d525412da29f263f17bfa375",
            "c0f360b4323e44e1a33644b47c5efd1f",
            "bcadfb805e4747b1b97c367ef337290b",
            "cf43d2a1b5d043a79f85fd21499a3094",
            "ec51851b5c8d4db49b301a075eeba4f3",
            "4b2bd485c4af4b8eacefacb2760793dd",
            "14fe9ed2df1d40778f4ebb869e6ce2c3",
            "4a0b10d2ac1443ffb04bec92b8852abc",
            "b6c8d04bd7024f7d997431b82856cf6a",
            "770ac17a361d41329947082dabc2ed72",
            "21799559dc5a4ac9816bfc85067252ac",
            "526fff2cb5324ec8a8ab039e1be5b865",
            "78e118e2feaf4e289611e8629aefea42",
            "95faa6a8c5b84067b8383fb5faa370e1",
            "7c21075bc39048d19e612e0816c6793d",
            "a433be6fc5734d82896d036e1f102479",
            "93e03d101c4b479fa516fbcb6aaee59c",
            "a49cb5fb1cc546f789960d54fac8a9ed",
            "37e542150ba943cb9894eaa4549a3f69",
            "fa1c1c0f55eb49fcbc57bce707a30c8e",
            "7cd79c85d6b1499b8470ad3ad83b92e5",
            "812510b0e421456792b865607cc09dd6",
            "926f08ab8278466ba5106761273b5050",
            "e3007a5d645941f39b713e65799a408b",
            "3613c80ee0294bac909983fbcef6a372",
            "b20b4a58d438481d87c4b19a7b915547",
            "9cbf69105df44c539d973aecf7f01b04",
            "729efbaf45a14428b4ea312c51d478f9",
            "9474f1a631bd4e4f878d818110ea17bc",
            "db0f1f70acf9454a9c4d74c9ad128408",
            "d1e3b94b9ef645b8a891b358fd746dd9",
            "31d35fed31104649b6b533ff1c9fa882",
            "0cb320a643d046898d9f0e68f1ae5fbd",
            "6beaa1bbec9c4337a74187b9a7bf4106"
          ]
        },
        "id": "OaM6Ql5u6gXd",
        "outputId": "43a7131f-bcb1-4263-92b1-824be948f19d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "adapter_config.json:   0%|          | 0.00/768 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bb4bbd3192c84c25845e331057a210f0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "027ce99a37de42a0bf76a91eecf29ed2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba673d6169644abc94cde721fc380b26"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5d574665011c4d3084de12dc41be2da5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7bcc1e2eb90b453a9be703ebc2d97343"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3594790a791e4547b607e4925fee4edf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c0f360b4323e44e1a33644b47c5efd1f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "78e118e2feaf4e289611e8629aefea42"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e3007a5d645941f39b713e65799a408b"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "# f_model = PeftModel.from_pretrained(model,'/kaggle/working/llama_finetuned_mcq/final_model')\n",
        "f_model = PeftModel.from_pretrained(\"Lakshya388/results\")\n",
        "\n",
        "f_model = f_model.merge_and_unload()"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "XnxUlDTa6gXd",
        "outputId": "83c59b50-1b90-4f9c-dfd4-72ab26648b75"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "PeftModel.from_pretrained() missing 1 required positional argument: 'model_id'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-698651a4e83d>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpeft\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPeftModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# f_model = PeftModel.from_pretrained(model,'/kaggle/working/llama_finetuned_mcq/final_model')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPeftModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Lakshya388/results\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_and_unload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: PeftModel.from_pretrained() missing 1 required positional argument: 'model_id'"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "# Ensure model is in evaluation mode\n",
        "f_model.eval()\n",
        "\n",
        "# Prepare for predictions\n",
        "predictions = []\n",
        "true_answer_indices = []  # To store true answer indices\n",
        "device = next(f_model.parameters()).device  # Ensure device compatibility\n",
        "batch_size = 1  # Adjust batch size as needed\n",
        "\n",
        "# Use DataLoader for batching if batch_size > 1\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset_custom, batch_size=batch_size)\n",
        "\n",
        "# Iterate through the test dataset\n",
        "for batch in tqdm(test_dataloader, desc=\"Generating predictions\", unit=\"batch\"):\n",
        "    # Move input tensors to the same device as the model\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "    # Generate outputs from the model\n",
        "    outputs = f_model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=10)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Append predictions\n",
        "    predictions.append(response)\n",
        "\n",
        "    # Append true answer index\n",
        "    true_idx = batch['answer_idx'][0].item()  # Assuming `answer_idx` stores the true indices\n",
        "    true_answer_indices.append(true_idx)\n",
        "\n",
        "# Save predictions and true indices to a single CSV file\n",
        "output_df = pd.DataFrame({\n",
        "    \"Predicted Output\": predictions,\n",
        "    \"True Answer Index\": true_answer_indices\n",
        "})\n",
        "output_df.to_csv(\"/kaggle/working/final_evaluation_results.csv\", index=False)\n",
        "\n",
        "print(\"Evaluation results saved to /kaggle/working/evaluation_results.csv\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "rmOXfq5b6gXd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "712e4f97-bc00-4e3f-b03a-341f35416a57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:   0%|          | 0/1273 [00:00<?, ?batch/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   0%|          | 0/1273 [00:03<?, ?batch/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'answer_idx' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-a775ae319c34>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# Append true answer index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mtrue_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0manswer_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Assuming `answer_idx` stores the true indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mtrue_answer_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'answer_idx' is not defined"
          ]
        }
      ],
      "execution_count": 13
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers accelerate bitsandbytes\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHXL6ryu_dMQ",
        "outputId": "a3d252b0-8023-4501-9422-a08d7d92d52c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.1.1)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.44.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.1+cu121)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "question = \"If you had a time machine, but could only go to the past or the future once and never return, which would you choose and why?\"\n",
        "from transformers import pipeline\n",
        "generator = pipeline(\"text-generation\", model=\"Lakshya388/results\", device=\"cuda\", torch_dtype=\"float16\")\n",
        "output = generator([{\"role\": \"user\", \"content\": question}], max_new_tokens=10, return_full_text=False)[0]\n",
        "print(output[\"generated_text\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178,
          "referenced_widgets": [
            "e170f241a2e24b3b9730f404ee1d80ac",
            "91a1cf7cb6354cf8910d59a42370443b",
            "e31de9ba843b443bb87ee14be68a2938",
            "bf3cea062e5949fe82f9bdb4f0c355b8",
            "e7466e57f5004c578f3c6fdb04698861",
            "90f618a232614be98106e0fccfa91d8d",
            "67842758c5d345ce8a2283e5ed928919",
            "575a9aef814f465599a21e1afb68e19f",
            "4cb03b9e952449e09e887894c151c78b",
            "c6f5abf26cfa4e29a1d0ccaef4ba4a07",
            "3c0023edd2a74b19b082a361e0c1812a"
          ]
        },
        "id": "hsZHhoey-IAl",
        "outputId": "4c3dda4b-61b1-4086-ab47-3630b574610a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e170f241a2e24b3b9730f404ee1d80ac"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load the model with int8 quantization\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Lakshya388/results\", device_map=\"auto\", load_in_8bit=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Lakshya388/results\")\n",
        "\n",
        "# Create the generator pipeline\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=\"cuda\")\n",
        "\n",
        "# Define the question\n",
        "question = \"If you had a time machine, but could only go to the past or the future once and never return, which would you choose and why?\"\n",
        "\n",
        "# Generate output\n",
        "output = generator([{\"role\": \"user\", \"content\": question}], max_new_tokens=10, return_full_text=False)[0]\n",
        "\n",
        "# Print the generated text\n",
        "print(output[\"generated_text\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 747,
          "referenced_widgets": [
            "114fc82f96424289935b7d9ad804e32c",
            "98ce82ea2e18476c89a853302fb5dd71",
            "cfe4ea58953a44edb8d490424505f7e0",
            "e057bf5ccf1e4e929669c2264b0db57a",
            "9d47c73eeb51417f8297c2be5b366535",
            "cd0e86679f5d467fa965847a712f3139",
            "c5f302e9dd54482db5997d8e9a32631d",
            "ca7b6749979e4b16960d48e9b47bf48b",
            "ead2090afdbb46cc924942f608a51733",
            "7556196966854524b39f0a1df2ac3430",
            "f91efa3ca6f4408e956785b3f8125414",
            "646f0ef629ba452081ecc97efe0fe137",
            "0fc8ec620ea7497986c1f9b0512b6a0f",
            "68b29981bfe44749b542c4eacf08d5eb",
            "6fecec50372441e9a2f91911905251f6",
            "f9c48257923241d7898032d45167d2b8",
            "39fd9bcced31413a9b72d226844ebd13",
            "9b6183ffe3644dbc932965a0c9ff1516",
            "faa2ee602796441b9158cecceaabe64f",
            "f7ae0088b97c4d2eacacdf1b8663ad82",
            "39d3e36ef78c487998c81275d6d4d8a0",
            "31e713c992484547b6025baa5f0b94c4",
            "39c1b8b4c53e44e484cd7f7e873c98a0",
            "2e286e5faf2f4eba96a0c06e98db8c4f",
            "8eb0eb8e68c5442fb046b35529a6b652",
            "7ff7427e38a141d48ae4d8a958e1765c",
            "ab60028b192346fa878b268bdfba47d4",
            "1025ae8b0728485ca4dc1de9d0828899",
            "54a4a4dda4dc4f15811f0588f588c798",
            "eaedd96341564798a1a25a00ac32cf9b",
            "41d0ecef14024e98920fff2c1bd7878a",
            "221f2e7b9b834d1aa2b44b167252748d",
            "c4200656dd08414883ffdfeedd00eb45",
            "4a213757d51249fa95cef80175a30c0e",
            "277bbcaa8710466eadabc51966c01227",
            "c2c5f1f06ae04c839af15bbb286e926d",
            "3ad2139ea4fb4393a8dc14ed7081edbf",
            "ee6eb05a97524faba51c457dc60d2626",
            "e4ba92138644472fbf20f065e029a262",
            "b23148b8806543d9bc593c778bba11a1",
            "6cfaee70391e4b1983d82e6e3a1ef768",
            "0affc9dbb6294436b482957694c1b62c",
            "437dc0ec3a9c44cfb9b1952104335ef7",
            "ee8e2eb7dd52428999873ef57bd26a3d",
            "e0a1023a344b45868c1de253ce50ebb8",
            "f51c77c2d3e545cea993f0095225ceb0",
            "3f614f2b83344d5e9d33106078dfac5c",
            "97cf7b4da9784879ad4872933445bcc9",
            "962147aa23854b0ab7910b04eaf36e51",
            "b638394d8d6e498fb6a9b813405625bb",
            "b6a3dc84a7094eb8ba92fd9b7322af55",
            "8eef344b95db460f8b4e0d84f35d0064",
            "1af5fc3fcc9a4efe83eb3c7f16cb2a71",
            "c0424b3f3d20439081bf7a7cf484b774",
            "b238a3ea65444706b10d7b94cac7591e",
            "43db7c9194054331b4191f1fd5992bc8",
            "e080237748c246f3bec88bb2cdacba46",
            "b62c77da20b4468ab1e5ff9bfd6b22e4",
            "30e6cb117b714f9ea120071ca896af2e",
            "61e2f66f2659484e89d266adcbac53c2",
            "3955223c44824d889930e7e255fc2fd9",
            "8e33a6b8b22a4130b656c299f988e28f",
            "7b0c8da98f1c490a960ba8520734e095",
            "08e8d081b3bd4edbb89b3e86ba5e6d56",
            "2d7b2b2413fa4c9b8e7244df76f4d3e9",
            "9a9038b418844797970d8a3b9c0238ce"
          ]
        },
        "id": "78ZosQ5n_hR7",
        "outputId": "be671af6-7673-45cc-81bf-e31f9a20dd0b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "114fc82f96424289935b7d9ad804e32c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "646f0ef629ba452081ecc97efe0fe137"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/2.45G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "39c1b8b4c53e44e484cd7f7e873c98a0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4a213757d51249fa95cef80175a30c0e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e0a1023a344b45868c1de253ce50ebb8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/325 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "43db7c9194054331b4191f1fd5992bc8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "The model has been loaded with `accelerate` and therefore cannot be moved to a specific device. Please discard the `device` argument when creating your pipeline object.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a663e7dd8a36>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Create the generator pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text-generation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Define the question\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/__init__.py\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"processor\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1164\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpipeline_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframework\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         self.check_model_type(\n\u001b[1;32m     98\u001b[0m             \u001b[0mTF_MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tf\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mMODEL_FOR_CAUSAL_LM_MAPPING_NAMES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, tokenizer, feature_extractor, image_processor, processor, modelcard, framework, task, args_parser, device, torch_dtype, binary_output, **kwargs)\u001b[0m\n\u001b[1;32m    872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhf_device_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    875\u001b[0m                 \u001b[0;34m\"The model has been loaded with `accelerate` and therefore cannot be moved to a specific device. Please \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m                 \u001b[0;34m\"discard the `device` argument when creating your pipeline object.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The model has been loaded with `accelerate` and therefore cannot be moved to a specific device. Please discard the `device` argument when creating your pipeline object."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "# f_model = PeftModel.from_pretrained(model,'/kaggle/working/llama_finetuned_mcq/final_model')\n",
        "f_model = PeftModel.from_pretrained(model,\"Lakshya388/results\")\n",
        "\n",
        "f_model = f_model.merge_and_unload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORkpO9COAE-w",
        "outputId": "844733e8-05bc-43d9-9012-1087ea975437"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/peft/tuners/lora/bnb.py:83: UserWarning: Merge lora module to 8-bit linear may get different generations due to rounding errors.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "# Ensure model is in evaluation mode\n",
        "f_model.eval()\n",
        "\n",
        "# Prepare for predictions\n",
        "predictions = []\n",
        "true_answer_indices = []  # To store true answer indices\n",
        "device = next(f_model.parameters()).device  # Ensure device compatibility\n",
        "\n",
        "# Iterate through the test dataset (assuming test_dataset_custom is a list or dataset)\n",
        "for item in tqdm(test_dataset_custom, desc=\"Generating predictions\", unit=\"item\"):\n",
        "    # Extract input tensors and move to the same device as the model\n",
        "    input_ids = torch.tensor(item['input_ids']).unsqueeze(0).to(device)  # Add batch dimension\n",
        "    attention_mask = torch.tensor(item['attention_mask']).unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "    # Generate outputs from the model\n",
        "    outputs = f_model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=10)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Append predictions\n",
        "    predictions.append(response)\n",
        "\n",
        "    # Append true answer index (if applicable)\n",
        "    true_answer_indices.append(item['answer_idx'])\n",
        "\n",
        "# Save predictions and true indices to a single CSV file\n",
        "output_df = pd.DataFrame({\n",
        "    \"Predicted Output\": predictions,\n",
        "    \"True Answer Index\": true_answer_indices\n",
        "})\n",
        "output_df.to_csv(\"final_evaluation_results.csv\", index=False)\n",
        "\n",
        "print(\"Evaluation results saved to final_evaluation_results.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "XX9iRWwIBiAB",
        "outputId": "e163c8f8-30b6-4cf7-ec5f-2982b0c228e3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating predictions:   0%|          | 0/1273 [00:00<?, ?item/s]<ipython-input-14-6ecc52066103>:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  input_ids = torch.tensor(item['input_ids']).unsqueeze(0).to(device)  # Add batch dimension\n",
            "<ipython-input-14-6ecc52066103>:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  attention_mask = torch.tensor(item['attention_mask']).unsqueeze(0).to(device)  # Add batch dimension\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   0%|          | 0/1273 [00:04<?, ?item/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'answer_idx'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-6ecc52066103>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Append true answer index (if applicable)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mtrue_answer_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'answer_idx'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Save predictions and true indices to a single CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'answer_idx'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_dataset_custom[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RM_oFifUBnsK",
        "outputId": "bfbd5fde-c9fd-407b-fab7-e5a3329f3caa"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([128000,  16533,    279,   2768,   3488,    555,  13758,    832,   4495,\n",
            "         35876,    907,    512,  14924,    551,    362,  27144,  30299,  40952,\n",
            "         34383,  15173,  19504,    374,  27666,    264,   1841,  19866,  26711,\n",
            "         13023,    449,    279,   9476,  21892,    439,    279,  24096,  28378,\n",
            "            13,  12220,    279,   1162,     11,    279,  19504,  70185,  15455,\n",
            "           264,   5882,    269,  88932,     13,    578,  88932,    374,  52834,\n",
            "          2085,  86919,     13,    578,  24096,  10975,    279,  19504,    430,\n",
            "           279,   8893,    690,    656,   7060,     11,    323,   1070,    374,\n",
            "           912,   1205,    311,   1934,    420,   9099,  86919,    430,    690,\n",
            "           539,  11682,    279,   8893,     11,    439,    568,   1587,    539,\n",
            "          1390,    311,   1304,    279,   8893,  11196,  83474,     13,   1283,\n",
            "         10975,    279,  19504,    311,   5387,    420,  86919,    704,    315,\n",
            "           279,  64885,   1934,     13,  16299,    315,    279,   2768,    374,\n",
            "           279,   4495,   1828,   1957,    369,    279,  19504,    311,   1935,\n",
            "          5380,   5454,    220,     16,     25,  11997,   1497,    279,   1493,\n",
            "           311,    279,   8893,    323,   2231,    433,    304,    279,  64885,\n",
            "          1934,    198,   5454,    220,     17,     25,  25672,    279,  24096,\n",
            "           430,    568,   4250,   3775,    311,  36333,    420,  16930,    198,\n",
            "          5454,    220,     18,     25,   8423,    279,  28378,    311,    279,\n",
            "         32008,  13093,    198,   5454,    220,     19,     25,   8718,    817,\n",
            "           311,  62974,    279,  64885,   1934,    198,  16533,   1193,    832,\n",
            "         35876,    907,     13,    720,  16533,     25, 128009, 128009, 128009,\n",
            "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
            "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
            "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
            "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
            "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
            "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
            "        128009, 128009, 128009, 128009], device='cuda:0'), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor(2, device='cuda:0')}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "# Ensure model is in evaluation mode\n",
        "f_model.eval()\n",
        "\n",
        "# Prepare for predictions\n",
        "predictions = []\n",
        "true_labels = []  # To store true labels\n",
        "device = next(f_model.parameters()).device  # Ensure device compatibility\n",
        "\n",
        "# Iterate through the test dataset\n",
        "for item in tqdm(test_dataset_custom, desc=\"Generating predictions\", unit=\"item\"):\n",
        "    # Extract input tensors and move to the same device as the model\n",
        "    input_ids = item['input_ids'].unsqueeze(0).to(device)  # Add batch dimension\n",
        "    attention_mask = item['attention_mask'].unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "    # Generate outputs from the model\n",
        "    outputs = f_model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=10)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Append predictions\n",
        "    predictions.append(response)\n",
        "\n",
        "    # Append true label\n",
        "    true_labels.append(item['labels'].item())  # Extract the true label\n",
        "\n",
        "# Save predictions and true labels to a single CSV file\n",
        "output_df = pd.DataFrame({\n",
        "    \"Predicted Output\": predictions,\n",
        "    \"True Label\": true_labels\n",
        "})\n",
        "output_df.to_csv(\"final_evaluation_results.csv\", index=False)\n",
        "\n",
        "print(\"Evaluation results saved to final_evaluation_results.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFT_nyIGBw44",
        "outputId": "765715b6-4894-4810-aa7e-1f69fd0a96ef"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:   0%|          | 0/1273 [00:00<?, ?item/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   0%|          | 1/1273 [00:03<1:07:36,  3.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   0%|          | 2/1273 [00:08<1:28:58,  4.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   0%|          | 3/1273 [00:11<1:17:11,  3.65s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   0%|          | 4/1273 [00:13<1:04:43,  3.06s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   0%|          | 5/1273 [00:15<57:03,  2.70s/item]  Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   0%|          | 6/1273 [00:17<52:34,  2.49s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   1%|          | 7/1273 [00:20<54:18,  2.57s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   1%|          | 8/1273 [00:22<51:06,  2.42s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   1%|          | 9/1273 [00:24<48:58,  2.32s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   1%|          | 10/1273 [00:26<47:05,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   1%|          | 11/1273 [00:28<47:01,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   1%|          | 12/1273 [00:31<48:21,  2.30s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   1%|          | 13/1273 [00:33<49:59,  2.38s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   1%|          | 14/1273 [00:35<47:43,  2.27s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   1%|          | 15/1273 [00:37<46:05,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   1%|▏         | 16/1273 [00:39<44:42,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   1%|▏         | 17/1273 [00:41<45:18,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   1%|▏         | 18/1273 [00:44<48:05,  2.30s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   1%|▏         | 19/1273 [00:48<59:15,  2.84s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   2%|▏         | 20/1273 [00:50<54:07,  2.59s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   2%|▏         | 21/1273 [00:52<50:25,  2.42s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   2%|▏         | 22/1273 [00:54<48:29,  2.33s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   2%|▏         | 23/1273 [00:57<48:33,  2.33s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   2%|▏         | 24/1273 [00:59<49:34,  2.38s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   2%|▏         | 25/1273 [01:01<48:16,  2.32s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   2%|▏         | 26/1273 [01:03<46:00,  2.21s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   2%|▏         | 27/1273 [01:05<44:30,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   2%|▏         | 28/1273 [01:07<44:24,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   2%|▏         | 29/1273 [01:10<47:59,  2.32s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   2%|▏         | 30/1273 [01:12<48:05,  2.32s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   2%|▏         | 31/1273 [01:15<47:09,  2.28s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   3%|▎         | 32/1273 [01:17<45:22,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   3%|▎         | 33/1273 [01:19<44:09,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   3%|▎         | 34/1273 [01:21<43:41,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   3%|▎         | 35/1273 [01:24<49:42,  2.41s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   3%|▎         | 36/1273 [01:26<47:24,  2.30s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   3%|▎         | 37/1273 [01:28<45:47,  2.22s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   3%|▎         | 38/1273 [01:30<44:29,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   3%|▎         | 39/1273 [01:32<44:12,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   3%|▎         | 40/1273 [01:34<44:05,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   3%|▎         | 41/1273 [01:37<48:45,  2.37s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   3%|▎         | 42/1273 [01:39<47:02,  2.29s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   3%|▎         | 43/1273 [01:41<46:09,  2.25s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   3%|▎         | 44/1273 [01:43<44:53,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   4%|▎         | 45/1273 [01:45<44:22,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   4%|▎         | 46/1273 [01:48<45:06,  2.21s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   4%|▎         | 47/1273 [01:50<47:09,  2.31s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   4%|▍         | 48/1273 [01:52<46:24,  2.27s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   4%|▍         | 49/1273 [01:55<44:57,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   4%|▍         | 50/1273 [01:57<43:55,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   4%|▍         | 51/1273 [01:59<42:47,  2.10s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   4%|▍         | 52/1273 [02:01<46:29,  2.28s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   4%|▍         | 53/1273 [02:03<45:32,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   4%|▍         | 54/1273 [02:05<44:30,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   4%|▍         | 55/1273 [02:07<43:35,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   4%|▍         | 56/1273 [02:10<42:59,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   4%|▍         | 57/1273 [02:12<42:44,  2.11s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   5%|▍         | 58/1273 [02:14<47:08,  2.33s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   5%|▍         | 59/1273 [02:17<46:18,  2.29s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   5%|▍         | 60/1273 [02:19<44:35,  2.21s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   5%|▍         | 61/1273 [02:21<43:43,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   5%|▍         | 62/1273 [02:23<43:13,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   5%|▍         | 63/1273 [02:25<43:57,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   5%|▌         | 64/1273 [02:28<46:43,  2.32s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   5%|▌         | 65/1273 [02:30<45:08,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   5%|▌         | 66/1273 [02:32<44:35,  2.22s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   5%|▌         | 67/1273 [02:34<43:38,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   5%|▌         | 68/1273 [02:36<43:27,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   5%|▌         | 69/1273 [02:39<46:40,  2.33s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   5%|▌         | 70/1273 [02:41<47:07,  2.35s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   6%|▌         | 71/1273 [02:43<45:43,  2.28s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   6%|▌         | 72/1273 [02:45<43:57,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   6%|▌         | 73/1273 [02:47<43:11,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   6%|▌         | 74/1273 [02:50<42:38,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   6%|▌         | 75/1273 [02:52<46:43,  2.34s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   6%|▌         | 76/1273 [02:55<45:38,  2.29s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   6%|▌         | 77/1273 [02:57<43:45,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   6%|▌         | 78/1273 [02:59<42:28,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   6%|▌         | 79/1273 [03:01<41:48,  2.10s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   6%|▋         | 80/1273 [03:03<42:25,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   6%|▋         | 81/1273 [03:06<46:32,  2.34s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   6%|▋         | 82/1273 [03:08<44:27,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   7%|▋         | 83/1273 [03:10<43:16,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   7%|▋         | 84/1273 [03:12<42:35,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   7%|▋         | 85/1273 [03:14<42:33,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   7%|▋         | 86/1273 [03:16<42:09,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   7%|▋         | 87/1273 [03:19<45:55,  2.32s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   7%|▋         | 88/1273 [03:21<44:38,  2.26s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   7%|▋         | 89/1273 [03:23<43:55,  2.23s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   7%|▋         | 90/1273 [03:25<43:09,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   7%|▋         | 91/1273 [03:27<42:20,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   7%|▋         | 92/1273 [03:30<43:58,  2.23s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   7%|▋         | 93/1273 [03:32<46:00,  2.34s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   7%|▋         | 94/1273 [03:34<44:44,  2.28s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   7%|▋         | 95/1273 [03:36<43:35,  2.22s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   8%|▊         | 96/1273 [03:38<42:12,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   8%|▊         | 97/1273 [03:41<42:05,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   8%|▊         | 98/1273 [03:43<44:31,  2.27s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   8%|▊         | 99/1273 [03:46<45:27,  2.32s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   8%|▊         | 100/1273 [03:48<44:04,  2.25s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   8%|▊         | 101/1273 [03:50<42:56,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   8%|▊         | 102/1273 [03:52<42:29,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   8%|▊         | 103/1273 [03:54<41:24,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   8%|▊         | 104/1273 [03:57<44:58,  2.31s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   8%|▊         | 105/1273 [03:59<45:03,  2.31s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   8%|▊         | 106/1273 [04:01<43:27,  2.23s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   8%|▊         | 107/1273 [04:03<42:46,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   8%|▊         | 108/1273 [04:05<41:37,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   9%|▊         | 109/1273 [04:07<40:56,  2.11s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   9%|▊         | 110/1273 [04:10<45:55,  2.37s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   9%|▊         | 111/1273 [04:12<45:09,  2.33s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   9%|▉         | 112/1273 [04:14<44:04,  2.28s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   9%|▉         | 113/1273 [04:17<43:23,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   9%|▉         | 114/1273 [04:19<42:46,  2.21s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   9%|▉         | 115/1273 [04:21<44:23,  2.30s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   9%|▉         | 116/1273 [04:24<44:44,  2.32s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   9%|▉         | 117/1273 [04:26<43:07,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   9%|▉         | 118/1273 [04:28<42:25,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   9%|▉         | 119/1273 [04:30<41:26,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   9%|▉         | 120/1273 [04:32<40:56,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  10%|▉         | 121/1273 [04:35<44:13,  2.30s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  10%|▉         | 122/1273 [04:37<43:52,  2.29s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  10%|▉         | 123/1273 [04:39<42:54,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  10%|▉         | 124/1273 [04:41<42:26,  2.22s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  10%|▉         | 125/1273 [04:43<41:26,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  10%|▉         | 126/1273 [04:45<41:01,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  10%|▉         | 127/1273 [04:48<44:13,  2.32s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  10%|█         | 128/1273 [04:50<42:52,  2.25s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  10%|█         | 129/1273 [04:52<41:44,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  10%|█         | 130/1273 [04:54<40:57,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  10%|█         | 131/1273 [04:56<40:49,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  10%|█         | 132/1273 [04:59<42:00,  2.21s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  10%|█         | 133/1273 [05:01<44:10,  2.32s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  11%|█         | 134/1273 [05:03<42:42,  2.25s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  11%|█         | 135/1273 [05:05<41:26,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  11%|█         | 136/1273 [05:07<40:23,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  11%|█         | 137/1273 [05:09<39:34,  2.09s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  11%|█         | 138/1273 [05:12<42:49,  2.26s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  11%|█         | 139/1273 [05:14<43:15,  2.29s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  11%|█         | 140/1273 [05:16<41:46,  2.21s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  11%|█         | 141/1273 [05:19<41:03,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  11%|█         | 142/1273 [05:21<40:15,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  11%|█         | 143/1273 [05:23<39:24,  2.09s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  11%|█▏        | 144/1273 [05:25<42:30,  2.26s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  11%|█▏        | 145/1273 [05:27<42:22,  2.25s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  11%|█▏        | 146/1273 [05:29<40:54,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  12%|█▏        | 147/1273 [05:31<39:51,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  12%|█▏        | 148/1273 [05:34<39:53,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  12%|█▏        | 149/1273 [05:36<39:04,  2.09s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  12%|█▏        | 150/1273 [05:38<42:26,  2.27s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  12%|█▏        | 151/1273 [05:41<42:53,  2.29s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  12%|█▏        | 152/1273 [05:43<41:34,  2.22s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  12%|█▏        | 153/1273 [05:45<40:25,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  12%|█▏        | 154/1273 [05:47<39:27,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  12%|█▏        | 155/1273 [05:49<38:56,  2.09s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  12%|█▏        | 156/1273 [05:52<43:16,  2.32s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  12%|█▏        | 157/1273 [05:54<42:44,  2.30s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  12%|█▏        | 158/1273 [05:56<41:38,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  12%|█▏        | 159/1273 [05:58<40:37,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  13%|█▎        | 160/1273 [06:00<40:00,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  13%|█▎        | 161/1273 [06:02<41:00,  2.21s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  13%|█▎        | 162/1273 [06:05<43:45,  2.36s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  13%|█▎        | 163/1273 [06:07<42:13,  2.28s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  13%|█▎        | 164/1273 [06:09<40:42,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  13%|█▎        | 165/1273 [06:11<39:34,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  13%|█▎        | 166/1273 [06:14<39:53,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  13%|█▎        | 167/1273 [06:16<40:34,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  13%|█▎        | 168/1273 [06:18<42:18,  2.30s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  13%|█▎        | 169/1273 [06:20<40:51,  2.22s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  13%|█▎        | 170/1273 [06:22<39:27,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  13%|█▎        | 171/1273 [06:24<38:51,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  14%|█▎        | 172/1273 [06:26<38:46,  2.11s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  14%|█▎        | 173/1273 [06:29<40:41,  2.22s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  14%|█▎        | 174/1273 [06:31<41:11,  2.25s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  14%|█▎        | 175/1273 [06:33<39:58,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  14%|█▍        | 176/1273 [06:35<39:54,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  14%|█▍        | 177/1273 [06:38<38:56,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  14%|█▍        | 178/1273 [06:40<38:16,  2.10s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  14%|█▍        | 179/1273 [06:42<41:48,  2.29s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  14%|█▍        | 180/1273 [06:44<40:16,  2.21s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  14%|█▍        | 181/1273 [06:46<39:22,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  14%|█▍        | 182/1273 [06:48<38:30,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  14%|█▍        | 183/1273 [06:50<37:58,  2.09s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  14%|█▍        | 184/1273 [06:52<38:03,  2.10s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  15%|█▍        | 185/1273 [06:55<42:39,  2.35s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  15%|█▍        | 186/1273 [06:58<41:19,  2.28s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  15%|█▍        | 187/1273 [07:00<39:34,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  15%|█▍        | 188/1273 [07:02<38:50,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  15%|█▍        | 189/1273 [07:04<38:15,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  15%|█▍        | 190/1273 [07:06<38:41,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  15%|█▌        | 191/1273 [07:09<41:55,  2.32s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  15%|█▌        | 192/1273 [07:11<40:18,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  15%|█▌        | 193/1273 [07:13<39:35,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  15%|█▌        | 194/1273 [07:15<38:27,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  15%|█▌        | 195/1273 [07:17<37:55,  2.11s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  15%|█▌        | 196/1273 [07:19<38:52,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  15%|█▌        | 197/1273 [07:22<41:29,  2.31s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  16%|█▌        | 198/1273 [07:24<40:20,  2.25s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  16%|█▌        | 199/1273 [07:26<39:30,  2.21s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  16%|█▌        | 200/1273 [07:28<38:23,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  16%|█▌        | 201/1273 [07:30<37:48,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  16%|█▌        | 202/1273 [07:33<40:16,  2.26s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  16%|█▌        | 203/1273 [07:35<40:56,  2.30s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  16%|█▌        | 204/1273 [07:37<39:38,  2.22s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  16%|█▌        | 205/1273 [07:39<38:30,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  16%|█▌        | 206/1273 [07:41<38:38,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  16%|█▋        | 207/1273 [07:43<38:09,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  16%|█▋        | 208/1273 [07:46<41:41,  2.35s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  16%|█▋        | 209/1273 [07:48<40:33,  2.29s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  16%|█▋        | 210/1273 [07:50<39:01,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  17%|█▋        | 211/1273 [07:52<38:04,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  17%|█▋        | 212/1273 [07:54<37:34,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  17%|█▋        | 213/1273 [07:56<37:17,  2.11s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  17%|█▋        | 214/1273 [07:59<41:22,  2.34s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  17%|█▋        | 215/1273 [08:01<39:53,  2.26s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  17%|█▋        | 216/1273 [08:04<39:00,  2.21s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  17%|█▋        | 217/1273 [08:06<38:33,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  17%|█▋        | 218/1273 [08:08<37:46,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  17%|█▋        | 219/1273 [08:10<37:40,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  17%|█▋        | 220/1273 [08:13<42:04,  2.40s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  17%|█▋        | 221/1273 [08:15<40:40,  2.32s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  17%|█▋        | 222/1273 [08:17<39:00,  2.23s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  18%|█▊        | 223/1273 [08:19<37:45,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  18%|█▊        | 224/1273 [08:21<36:54,  2.11s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  18%|█▊        | 225/1273 [08:23<37:34,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  18%|█▊        | 226/1273 [08:26<40:48,  2.34s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  18%|█▊        | 227/1273 [08:28<40:00,  2.29s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  18%|█▊        | 228/1273 [08:30<38:42,  2.22s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  18%|█▊        | 229/1273 [08:32<37:56,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  18%|█▊        | 230/1273 [08:34<37:29,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  18%|█▊        | 231/1273 [08:37<39:01,  2.25s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  18%|█▊        | 232/1273 [08:39<40:14,  2.32s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  18%|█▊        | 233/1273 [08:41<38:37,  2.23s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  18%|█▊        | 234/1273 [08:43<37:52,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  18%|█▊        | 235/1273 [08:45<36:56,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  19%|█▊        | 236/1273 [08:47<36:14,  2.10s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  19%|█▊        | 237/1273 [08:50<38:52,  2.25s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  19%|█▊        | 238/1273 [08:53<39:46,  2.31s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  19%|█▉        | 239/1273 [08:55<38:21,  2.23s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  19%|█▉        | 240/1273 [08:57<37:11,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  19%|█▉        | 241/1273 [08:59<36:46,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  19%|█▉        | 242/1273 [09:01<35:54,  2.09s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  19%|█▉        | 243/1273 [09:03<38:41,  2.25s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  19%|█▉        | 244/1273 [09:06<38:55,  2.27s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  19%|█▉        | 245/1273 [09:08<38:15,  2.23s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  19%|█▉        | 246/1273 [09:10<37:42,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  19%|█▉        | 247/1273 [09:12<36:58,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  19%|█▉        | 248/1273 [09:14<37:17,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  20%|█▉        | 249/1273 [09:17<40:06,  2.35s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  20%|█▉        | 250/1273 [09:19<38:35,  2.26s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  20%|█▉        | 251/1273 [09:21<37:27,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  20%|█▉        | 252/1273 [09:23<36:29,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  20%|█▉        | 253/1273 [09:25<35:40,  2.10s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  20%|█▉        | 254/1273 [09:27<37:19,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  20%|██        | 255/1273 [09:30<39:22,  2.32s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  20%|██        | 256/1273 [09:32<38:00,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  20%|██        | 257/1273 [09:34<36:51,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  20%|██        | 258/1273 [09:36<35:55,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  20%|██        | 259/1273 [09:38<35:13,  2.08s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  20%|██        | 260/1273 [09:41<36:54,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  21%|██        | 261/1273 [09:43<37:48,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  21%|██        | 262/1273 [09:45<37:22,  2.22s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  21%|██        | 263/1273 [09:47<36:38,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  21%|██        | 264/1273 [09:49<35:50,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  21%|██        | 265/1273 [09:51<35:38,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  21%|██        | 266/1273 [09:54<38:54,  2.32s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  21%|██        | 267/1273 [09:56<38:19,  2.29s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  21%|██        | 268/1273 [09:58<36:40,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  21%|██        | 269/1273 [10:00<35:21,  2.11s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  21%|██        | 270/1273 [10:02<34:42,  2.08s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  21%|██▏       | 271/1273 [10:04<34:15,  2.05s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  21%|██▏       | 272/1273 [10:07<38:30,  2.31s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  21%|██▏       | 273/1273 [10:09<37:29,  2.25s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  22%|██▏       | 274/1273 [10:11<36:31,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  22%|██▏       | 275/1273 [10:13<35:43,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  22%|██▏       | 276/1273 [10:15<35:27,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  22%|██▏       | 277/1273 [10:17<35:06,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  22%|██▏       | 278/1273 [10:20<38:46,  2.34s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  22%|██▏       | 279/1273 [10:22<37:03,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  22%|██▏       | 280/1273 [10:24<36:17,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  22%|██▏       | 281/1273 [10:26<35:20,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  22%|██▏       | 282/1273 [10:28<34:58,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  22%|██▏       | 283/1273 [10:31<35:15,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  22%|██▏       | 284/1273 [10:34<38:48,  2.35s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  22%|██▏       | 285/1273 [10:36<37:58,  2.31s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  22%|██▏       | 286/1273 [10:38<37:05,  2.25s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  23%|██▎       | 287/1273 [10:40<36:01,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  23%|██▎       | 288/1273 [10:42<35:32,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  23%|██▎       | 289/1273 [10:44<36:10,  2.21s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  23%|██▎       | 290/1273 [10:47<37:15,  2.27s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  23%|██▎       | 291/1273 [10:49<36:08,  2.21s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  23%|██▎       | 292/1273 [10:51<34:49,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  23%|██▎       | 293/1273 [10:53<34:19,  2.10s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  23%|██▎       | 294/1273 [10:55<33:41,  2.06s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  23%|██▎       | 295/1273 [10:57<35:13,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  23%|██▎       | 296/1273 [11:00<36:59,  2.27s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  23%|██▎       | 297/1273 [11:02<35:55,  2.21s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  23%|██▎       | 298/1273 [11:04<35:34,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  23%|██▎       | 299/1273 [11:06<34:44,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  24%|██▎       | 300/1273 [11:08<34:02,  2.10s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  24%|██▎       | 301/1273 [11:10<35:23,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  24%|██▎       | 302/1273 [11:13<36:38,  2.26s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  24%|██▍       | 303/1273 [11:15<35:37,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  24%|██▍       | 304/1273 [11:17<34:40,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  24%|██▍       | 305/1273 [11:19<34:34,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  24%|██▍       | 306/1273 [11:21<33:57,  2.11s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  24%|██▍       | 307/1273 [11:24<37:58,  2.36s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  24%|██▍       | 308/1273 [11:26<36:55,  2.30s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  24%|██▍       | 309/1273 [11:28<35:39,  2.22s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  24%|██▍       | 310/1273 [11:30<35:09,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  24%|██▍       | 311/1273 [11:32<34:32,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  25%|██▍       | 312/1273 [11:34<34:15,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  25%|██▍       | 313/1273 [11:37<37:26,  2.34s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  25%|██▍       | 314/1273 [11:39<36:05,  2.26s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  25%|██▍       | 315/1273 [11:41<34:43,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  25%|██▍       | 316/1273 [11:43<33:52,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  25%|██▍       | 317/1273 [11:45<33:28,  2.10s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  25%|██▍       | 318/1273 [11:47<33:42,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  25%|██▌       | 319/1273 [11:50<36:24,  2.29s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  25%|██▌       | 320/1273 [11:52<35:36,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  25%|██▌       | 321/1273 [11:54<34:29,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  25%|██▌       | 322/1273 [11:56<33:45,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  25%|██▌       | 323/1273 [11:58<33:32,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  25%|██▌       | 324/1273 [12:01<35:18,  2.23s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  26%|██▌       | 325/1273 [12:03<36:21,  2.30s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  26%|██▌       | 326/1273 [12:05<34:57,  2.21s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  26%|██▌       | 327/1273 [12:08<34:37,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  26%|██▌       | 328/1273 [12:10<33:33,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  26%|██▌       | 329/1273 [12:12<32:47,  2.08s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  26%|██▌       | 330/1273 [12:14<35:36,  2.27s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  26%|██▌       | 331/1273 [12:16<34:47,  2.22s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  26%|██▌       | 332/1273 [12:18<34:18,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  26%|██▌       | 333/1273 [12:21<33:51,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  26%|██▌       | 334/1273 [12:23<34:06,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  26%|██▋       | 335/1273 [12:25<34:19,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  26%|██▋       | 336/1273 [12:28<37:02,  2.37s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  26%|██▋       | 337/1273 [12:30<36:04,  2.31s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  27%|██▋       | 338/1273 [12:32<34:33,  2.22s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  27%|██▋       | 339/1273 [12:34<33:54,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  27%|██▋       | 340/1273 [12:36<33:12,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  27%|██▋       | 341/1273 [12:38<34:35,  2.23s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  27%|██▋       | 342/1273 [12:41<36:15,  2.34s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  27%|██▋       | 343/1273 [12:43<34:43,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  27%|██▋       | 344/1273 [12:45<34:25,  2.22s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  27%|██▋       | 345/1273 [12:47<33:38,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  27%|██▋       | 346/1273 [12:49<32:38,  2.11s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  27%|██▋       | 347/1273 [12:52<33:36,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  27%|██▋       | 348/1273 [12:54<35:22,  2.30s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  27%|██▋       | 349/1273 [12:56<33:48,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  27%|██▋       | 350/1273 [12:58<33:20,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  28%|██▊       | 351/1273 [13:00<33:13,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  28%|██▊       | 352/1273 [13:03<32:56,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  28%|██▊       | 353/1273 [13:05<35:58,  2.35s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  28%|██▊       | 354/1273 [13:08<35:15,  2.30s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  28%|██▊       | 355/1273 [13:10<33:57,  2.22s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  28%|██▊       | 356/1273 [13:12<33:26,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  28%|██▊       | 357/1273 [13:14<32:42,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  28%|██▊       | 358/1273 [13:16<32:43,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  28%|██▊       | 359/1273 [13:19<35:34,  2.33s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  28%|██▊       | 360/1273 [13:21<34:05,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  28%|██▊       | 361/1273 [13:23<33:15,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  28%|██▊       | 362/1273 [13:25<32:41,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  29%|██▊       | 363/1273 [13:27<31:40,  2.09s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  29%|██▊       | 364/1273 [13:29<31:44,  2.10s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  29%|██▊       | 365/1273 [13:32<34:39,  2.29s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  29%|██▉       | 366/1273 [13:34<33:24,  2.21s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  29%|██▉       | 367/1273 [13:36<33:02,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  29%|██▉       | 368/1273 [13:38<32:47,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  29%|██▉       | 369/1273 [13:40<31:51,  2.11s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  29%|██▉       | 370/1273 [13:42<32:41,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  29%|██▉       | 371/1273 [13:45<34:54,  2.32s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  29%|██▉       | 372/1273 [13:47<33:59,  2.26s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  29%|██▉       | 373/1273 [13:49<33:33,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  29%|██▉       | 374/1273 [13:51<32:32,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  29%|██▉       | 375/1273 [13:53<32:12,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  30%|██▉       | 376/1273 [13:56<32:56,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  30%|██▉       | 377/1273 [13:58<35:01,  2.35s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  30%|██▉       | 378/1273 [14:00<33:45,  2.26s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  30%|██▉       | 379/1273 [14:02<32:51,  2.21s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  30%|██▉       | 380/1273 [14:05<32:31,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  30%|██▉       | 381/1273 [14:07<32:09,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  30%|███       | 382/1273 [14:09<34:25,  2.32s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  30%|███       | 383/1273 [14:12<33:56,  2.29s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  30%|███       | 384/1273 [14:14<33:10,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  30%|███       | 385/1273 [14:16<32:33,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  30%|███       | 386/1273 [14:18<31:37,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  30%|███       | 387/1273 [14:20<31:09,  2.11s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  30%|███       | 388/1273 [14:23<34:04,  2.31s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  31%|███       | 389/1273 [14:25<32:56,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  31%|███       | 390/1273 [14:27<31:48,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  31%|███       | 391/1273 [14:29<31:29,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  31%|███       | 392/1273 [14:31<31:05,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  31%|███       | 393/1273 [14:33<31:40,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  31%|███       | 394/1273 [14:36<33:34,  2.29s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  31%|███       | 395/1273 [14:38<32:41,  2.23s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  31%|███       | 396/1273 [14:40<31:48,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  31%|███       | 397/1273 [14:42<31:23,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  31%|███▏      | 398/1273 [14:44<30:54,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  31%|███▏      | 399/1273 [14:46<31:57,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  31%|███▏      | 400/1273 [14:49<33:43,  2.32s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  32%|███▏      | 401/1273 [14:51<33:08,  2.28s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  32%|███▏      | 402/1273 [14:53<32:25,  2.23s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  32%|███▏      | 403/1273 [14:55<31:45,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  32%|███▏      | 404/1273 [14:57<30:50,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  32%|███▏      | 405/1273 [15:00<33:33,  2.32s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  32%|███▏      | 406/1273 [15:02<33:37,  2.33s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  32%|███▏      | 407/1273 [15:04<31:57,  2.21s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  32%|███▏      | 408/1273 [15:06<30:51,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  32%|███▏      | 409/1273 [15:09<30:45,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  32%|███▏      | 410/1273 [15:11<30:14,  2.10s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  32%|███▏      | 411/1273 [15:13<32:47,  2.28s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  32%|███▏      | 412/1273 [15:15<31:57,  2.23s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  32%|███▏      | 413/1273 [15:18<31:46,  2.22s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  33%|███▎      | 414/1273 [15:20<31:30,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  33%|███▎      | 415/1273 [15:22<30:43,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  33%|███▎      | 416/1273 [15:24<31:09,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  33%|███▎      | 417/1273 [15:27<32:46,  2.30s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  33%|███▎      | 418/1273 [15:29<31:48,  2.23s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  33%|███▎      | 419/1273 [15:31<31:14,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  33%|███▎      | 420/1273 [15:33<30:39,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  33%|███▎      | 421/1273 [15:35<29:56,  2.11s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  33%|███▎      | 422/1273 [15:37<30:40,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  33%|███▎      | 423/1273 [15:40<32:53,  2.32s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  33%|███▎      | 424/1273 [15:42<32:20,  2.29s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  33%|███▎      | 425/1273 [15:44<31:22,  2.22s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  33%|███▎      | 426/1273 [15:46<30:45,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  34%|███▎      | 427/1273 [15:48<30:36,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  34%|███▎      | 428/1273 [15:51<32:48,  2.33s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  34%|███▎      | 429/1273 [15:53<32:40,  2.32s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  34%|███▍      | 430/1273 [15:55<31:20,  2.23s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  34%|███▍      | 431/1273 [15:57<30:13,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  34%|███▍      | 432/1273 [15:59<29:38,  2.11s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  34%|███▍      | 433/1273 [16:01<29:47,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  34%|███▍      | 434/1273 [16:04<32:46,  2.34s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  34%|███▍      | 435/1273 [16:06<31:44,  2.27s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  34%|███▍      | 436/1273 [16:08<30:46,  2.21s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  34%|███▍      | 437/1273 [16:10<29:34,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  34%|███▍      | 438/1273 [16:12<29:25,  2.11s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  34%|███▍      | 439/1273 [16:15<29:31,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  35%|███▍      | 440/1273 [16:17<32:06,  2.31s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  35%|███▍      | 441/1273 [16:19<30:55,  2.23s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  35%|███▍      | 442/1273 [16:21<30:01,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  35%|███▍      | 443/1273 [16:23<29:24,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  35%|███▍      | 444/1273 [16:26<29:14,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  35%|███▍      | 445/1273 [16:28<30:16,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  35%|███▌      | 446/1273 [16:31<32:25,  2.35s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  35%|███▌      | 447/1273 [16:33<31:23,  2.28s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  35%|███▌      | 448/1273 [16:35<30:13,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  35%|███▌      | 449/1273 [16:37<29:47,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  35%|███▌      | 450/1273 [16:39<29:06,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  35%|███▌      | 451/1273 [16:41<30:17,  2.21s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  36%|███▌      | 452/1273 [16:44<30:54,  2.26s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  36%|███▌      | 453/1273 [16:46<30:06,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  36%|███▌      | 454/1273 [16:48<29:47,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  36%|███▌      | 455/1273 [16:50<29:06,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  36%|███▌      | 456/1273 [16:52<28:20,  2.08s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  36%|███▌      | 457/1273 [16:55<30:37,  2.25s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  36%|███▌      | 458/1273 [16:57<30:45,  2.26s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  36%|███▌      | 459/1273 [16:59<29:39,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  36%|███▌      | 460/1273 [17:01<29:26,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  36%|███▌      | 461/1273 [17:03<29:23,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  36%|███▋      | 462/1273 [17:05<29:41,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  36%|███▋      | 463/1273 [17:08<32:02,  2.37s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  36%|███▋      | 464/1273 [17:10<30:57,  2.30s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  37%|███▋      | 465/1273 [17:12<30:32,  2.27s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  37%|███▋      | 466/1273 [17:14<29:28,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  37%|███▋      | 467/1273 [17:17<29:23,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  37%|███▋      | 468/1273 [17:19<30:31,  2.28s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  37%|███▋      | 469/1273 [17:22<31:49,  2.37s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  37%|███▋      | 470/1273 [17:24<31:04,  2.32s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  37%|███▋      | 471/1273 [17:26<29:48,  2.23s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  37%|███▋      | 472/1273 [17:28<29:07,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  37%|███▋      | 473/1273 [17:30<28:27,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  37%|███▋      | 474/1273 [17:33<30:41,  2.31s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  37%|███▋      | 475/1273 [17:35<29:36,  2.23s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  37%|███▋      | 476/1273 [17:37<28:36,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  37%|███▋      | 477/1273 [17:39<27:57,  2.11s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  38%|███▊      | 478/1273 [17:41<27:53,  2.10s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  38%|███▊      | 479/1273 [17:43<27:19,  2.06s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  38%|███▊      | 480/1273 [17:46<30:24,  2.30s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  38%|███▊      | 481/1273 [17:48<29:30,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  38%|███▊      | 482/1273 [17:50<28:46,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  38%|███▊      | 483/1273 [17:52<28:08,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  38%|███▊      | 484/1273 [17:54<28:03,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  38%|███▊      | 485/1273 [17:56<27:40,  2.11s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  38%|███▊      | 486/1273 [17:59<30:35,  2.33s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  38%|███▊      | 487/1273 [18:01<29:31,  2.25s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  38%|███▊      | 488/1273 [18:03<28:32,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  38%|███▊      | 489/1273 [18:05<28:01,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  38%|███▊      | 490/1273 [18:07<27:24,  2.10s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  39%|███▊      | 491/1273 [18:09<27:56,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  39%|███▊      | 492/1273 [18:12<30:12,  2.32s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  39%|███▊      | 493/1273 [18:14<29:32,  2.27s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  39%|███▉      | 494/1273 [18:16<29:04,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  39%|███▉      | 495/1273 [18:18<28:03,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  39%|███▉      | 496/1273 [18:20<27:29,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  39%|███▉      | 497/1273 [18:23<28:26,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  39%|███▉      | 498/1273 [18:25<29:50,  2.31s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  39%|███▉      | 499/1273 [18:28<29:20,  2.27s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  39%|███▉      | 500/1273 [18:30<28:52,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  39%|███▉      | 501/1273 [18:32<27:48,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  39%|███▉      | 502/1273 [18:34<27:52,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  40%|███▉      | 503/1273 [18:36<29:31,  2.30s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  40%|███▉      | 504/1273 [18:39<29:19,  2.29s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  40%|███▉      | 505/1273 [18:41<28:19,  2.21s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  40%|███▉      | 506/1273 [18:43<27:54,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  40%|███▉      | 507/1273 [18:45<27:01,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  40%|███▉      | 508/1273 [18:47<26:48,  2.10s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  40%|███▉      | 509/1273 [18:50<29:58,  2.35s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  40%|████      | 510/1273 [18:52<29:08,  2.29s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  40%|████      | 511/1273 [18:54<28:05,  2.21s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  40%|████      | 512/1273 [18:56<27:32,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  40%|████      | 513/1273 [18:58<26:59,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  40%|████      | 514/1273 [19:00<27:23,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  40%|████      | 515/1273 [19:03<29:49,  2.36s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  41%|████      | 516/1273 [19:05<28:28,  2.26s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  41%|████      | 517/1273 [19:07<27:33,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  41%|████      | 518/1273 [19:09<26:37,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  41%|████      | 519/1273 [19:11<26:41,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  41%|████      | 520/1273 [19:14<27:39,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  41%|████      | 521/1273 [19:16<29:09,  2.33s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  41%|████      | 522/1273 [19:18<28:26,  2.27s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  41%|████      | 523/1273 [19:21<27:44,  2.22s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  41%|████      | 524/1273 [19:23<26:56,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  41%|████      | 525/1273 [19:25<26:19,  2.11s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  41%|████▏     | 526/1273 [19:27<28:48,  2.31s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  41%|████▏     | 527/1273 [19:30<28:35,  2.30s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  41%|████▏     | 528/1273 [19:32<27:42,  2.23s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  42%|████▏     | 529/1273 [19:34<27:02,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  42%|████▏     | 530/1273 [19:36<26:12,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  42%|████▏     | 531/1273 [19:38<25:39,  2.08s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  42%|████▏     | 532/1273 [19:41<28:49,  2.33s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  42%|████▏     | 533/1273 [19:43<28:13,  2.29s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  42%|████▏     | 534/1273 [19:45<27:23,  2.22s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  42%|████▏     | 535/1273 [19:47<26:26,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  42%|████▏     | 536/1273 [19:49<26:48,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  42%|████▏     | 537/1273 [19:51<27:25,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  42%|████▏     | 538/1273 [19:54<28:30,  2.33s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  42%|████▏     | 539/1273 [19:56<27:59,  2.29s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  42%|████▏     | 540/1273 [19:58<26:51,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  42%|████▏     | 541/1273 [20:00<26:13,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  43%|████▎     | 542/1273 [20:02<25:46,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  43%|████▎     | 543/1273 [20:05<26:49,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  43%|████▎     | 544/1273 [20:07<28:09,  2.32s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  43%|████▎     | 545/1273 [20:09<26:57,  2.22s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  43%|████▎     | 546/1273 [20:11<26:09,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  43%|████▎     | 547/1273 [20:13<25:45,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  43%|████▎     | 548/1273 [20:15<25:29,  2.11s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  43%|████▎     | 549/1273 [20:18<27:46,  2.30s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  43%|████▎     | 550/1273 [20:20<27:30,  2.28s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  43%|████▎     | 551/1273 [20:22<26:28,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  43%|████▎     | 552/1273 [20:24<26:00,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  43%|████▎     | 553/1273 [20:27<26:12,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  44%|████▎     | 554/1273 [20:29<25:43,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  44%|████▎     | 555/1273 [20:32<28:04,  2.35s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  44%|████▎     | 556/1273 [20:34<26:46,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  44%|████▍     | 557/1273 [20:36<26:15,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  44%|████▍     | 558/1273 [20:38<25:45,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  44%|████▍     | 559/1273 [20:40<25:28,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  44%|████▍     | 560/1273 [20:42<26:00,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  44%|████▍     | 561/1273 [20:45<28:06,  2.37s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  44%|████▍     | 562/1273 [20:47<27:10,  2.29s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  44%|████▍     | 563/1273 [20:49<26:07,  2.21s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  44%|████▍     | 564/1273 [20:51<25:30,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  44%|████▍     | 565/1273 [20:53<24:58,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  44%|████▍     | 566/1273 [20:56<26:08,  2.22s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  45%|████▍     | 567/1273 [20:58<27:05,  2.30s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  45%|████▍     | 568/1273 [21:00<25:56,  2.21s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  45%|████▍     | 569/1273 [21:02<25:41,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  45%|████▍     | 570/1273 [21:04<25:10,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  45%|████▍     | 571/1273 [21:06<24:38,  2.11s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  45%|████▍     | 572/1273 [21:09<26:10,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  45%|████▌     | 573/1273 [21:11<26:37,  2.28s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  45%|████▌     | 574/1273 [21:13<25:45,  2.21s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  45%|████▌     | 575/1273 [21:15<25:01,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  45%|████▌     | 576/1273 [21:17<24:39,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  45%|████▌     | 577/1273 [21:19<24:39,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  45%|████▌     | 578/1273 [21:22<26:08,  2.26s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  45%|████▌     | 579/1273 [21:24<26:13,  2.27s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  46%|████▌     | 580/1273 [21:26<25:49,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  46%|████▌     | 581/1273 [21:29<25:20,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  46%|████▌     | 582/1273 [21:31<24:33,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  46%|████▌     | 583/1273 [21:33<23:58,  2.08s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  46%|████▌     | 584/1273 [21:35<26:45,  2.33s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  46%|████▌     | 585/1273 [21:38<26:02,  2.27s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  46%|████▌     | 586/1273 [21:40<25:01,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  46%|████▌     | 587/1273 [21:42<24:23,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  46%|████▌     | 588/1273 [21:44<23:54,  2.09s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  46%|████▋     | 589/1273 [21:46<24:12,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  46%|████▋     | 590/1273 [21:49<26:44,  2.35s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  46%|████▋     | 591/1273 [21:51<25:36,  2.25s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  47%|████▋     | 592/1273 [21:53<24:48,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  47%|████▋     | 593/1273 [21:55<24:04,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  47%|████▋     | 594/1273 [21:57<23:52,  2.11s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  47%|████▋     | 595/1273 [21:59<23:41,  2.10s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  47%|████▋     | 596/1273 [22:02<25:57,  2.30s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  47%|████▋     | 597/1273 [22:04<24:59,  2.22s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  47%|████▋     | 598/1273 [22:06<24:25,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  47%|████▋     | 599/1273 [22:08<24:08,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  47%|████▋     | 600/1273 [22:10<23:29,  2.09s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  47%|████▋     | 601/1273 [22:12<23:31,  2.10s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  47%|████▋     | 602/1273 [22:15<25:48,  2.31s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  47%|████▋     | 603/1273 [22:17<25:23,  2.27s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  47%|████▋     | 604/1273 [22:19<24:29,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  48%|████▊     | 605/1273 [22:21<23:58,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  48%|████▊     | 606/1273 [22:23<23:46,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  48%|████▊     | 607/1273 [22:25<24:06,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  48%|████▊     | 608/1273 [22:28<25:59,  2.34s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  48%|████▊     | 609/1273 [22:30<25:36,  2.31s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  48%|████▊     | 610/1273 [22:32<24:30,  2.22s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  48%|████▊     | 611/1273 [22:34<24:00,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  48%|████▊     | 612/1273 [22:36<23:32,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  48%|████▊     | 613/1273 [22:39<25:08,  2.29s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  48%|████▊     | 614/1273 [22:42<25:48,  2.35s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  48%|████▊     | 615/1273 [22:44<24:41,  2.25s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  48%|████▊     | 616/1273 [22:46<24:08,  2.21s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  48%|████▊     | 617/1273 [22:48<23:33,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  49%|████▊     | 618/1273 [22:50<23:04,  2.11s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  49%|████▊     | 619/1273 [22:52<24:35,  2.26s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  49%|████▊     | 620/1273 [22:54<24:15,  2.23s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  49%|████▉     | 621/1273 [22:56<23:36,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  49%|████▉     | 622/1273 [22:58<22:57,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  49%|████▉     | 623/1273 [23:00<22:28,  2.07s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  49%|████▉     | 624/1273 [23:03<22:22,  2.07s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  49%|████▉     | 625/1273 [23:05<23:59,  2.22s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  49%|████▉     | 626/1273 [23:07<23:49,  2.21s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  49%|████▉     | 627/1273 [23:09<23:24,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  49%|████▉     | 628/1273 [23:11<23:12,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  49%|████▉     | 629/1273 [23:14<22:58,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  49%|████▉     | 630/1273 [23:16<22:21,  2.09s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  50%|████▉     | 631/1273 [23:18<24:43,  2.31s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  50%|████▉     | 632/1273 [23:20<24:02,  2.25s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  50%|████▉     | 633/1273 [23:22<23:06,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  50%|████▉     | 634/1273 [23:25<22:50,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  50%|████▉     | 635/1273 [23:27<22:42,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  50%|████▉     | 636/1273 [23:29<22:09,  2.09s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  50%|█████     | 637/1273 [23:31<24:16,  2.29s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  50%|█████     | 638/1273 [23:33<23:37,  2.23s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  50%|█████     | 639/1273 [23:36<22:53,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  50%|█████     | 640/1273 [23:38<22:35,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  50%|█████     | 641/1273 [23:40<22:25,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  50%|█████     | 642/1273 [23:42<22:54,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  51%|█████     | 643/1273 [23:45<24:27,  2.33s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  51%|█████     | 644/1273 [23:47<23:30,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  51%|█████     | 645/1273 [23:49<22:40,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  51%|█████     | 646/1273 [23:51<22:00,  2.11s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  51%|█████     | 647/1273 [23:53<21:38,  2.07s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  51%|█████     | 648/1273 [23:55<21:39,  2.08s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  51%|█████     | 649/1273 [23:57<23:31,  2.26s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  51%|█████     | 650/1273 [23:59<22:41,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  51%|█████     | 651/1273 [24:01<21:59,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  51%|█████     | 652/1273 [24:03<21:39,  2.09s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  51%|█████▏    | 653/1273 [24:06<21:36,  2.09s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  51%|█████▏    | 654/1273 [24:08<22:14,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  51%|█████▏    | 655/1273 [24:10<23:38,  2.30s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  52%|█████▏    | 656/1273 [24:13<22:56,  2.23s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  52%|█████▏    | 657/1273 [24:15<22:08,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  52%|█████▏    | 658/1273 [24:17<21:57,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  52%|█████▏    | 659/1273 [24:19<21:38,  2.11s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  52%|█████▏    | 660/1273 [24:21<23:29,  2.30s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  52%|█████▏    | 661/1273 [24:24<23:21,  2.29s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  52%|█████▏    | 662/1273 [24:26<22:43,  2.23s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  52%|█████▏    | 663/1273 [24:28<22:09,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  52%|█████▏    | 664/1273 [24:30<21:49,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  52%|█████▏    | 665/1273 [24:32<21:31,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  52%|█████▏    | 666/1273 [24:35<23:30,  2.32s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  52%|█████▏    | 667/1273 [24:37<22:29,  2.23s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  52%|█████▏    | 668/1273 [24:39<22:07,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  53%|█████▎    | 669/1273 [24:41<21:29,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  53%|█████▎    | 670/1273 [24:43<21:17,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  53%|█████▎    | 671/1273 [24:45<21:32,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  53%|█████▎    | 672/1273 [24:48<23:26,  2.34s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  53%|█████▎    | 673/1273 [24:50<22:29,  2.25s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  53%|█████▎    | 674/1273 [24:52<22:09,  2.22s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  53%|█████▎    | 675/1273 [24:54<21:41,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  53%|█████▎    | 676/1273 [24:56<21:12,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  53%|█████▎    | 677/1273 [24:59<21:32,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  53%|█████▎    | 678/1273 [25:01<22:28,  2.27s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  53%|█████▎    | 679/1273 [25:03<22:08,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  53%|█████▎    | 680/1273 [25:05<21:28,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  53%|█████▎    | 681/1273 [25:07<21:18,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  54%|█████▎    | 682/1273 [25:09<21:13,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  54%|█████▎    | 683/1273 [25:12<22:42,  2.31s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  54%|█████▎    | 684/1273 [25:15<22:50,  2.33s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  54%|█████▍    | 685/1273 [25:17<22:01,  2.25s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  54%|█████▍    | 686/1273 [25:19<21:28,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  54%|█████▍    | 687/1273 [25:21<20:56,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  54%|█████▍    | 688/1273 [25:23<20:29,  2.10s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  54%|█████▍    | 689/1273 [25:26<22:43,  2.33s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  54%|█████▍    | 690/1273 [25:28<21:53,  2.25s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  54%|█████▍    | 691/1273 [25:30<21:26,  2.21s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  54%|█████▍    | 692/1273 [25:32<21:01,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  54%|█████▍    | 693/1273 [25:34<20:26,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  55%|█████▍    | 694/1273 [25:36<20:31,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  55%|█████▍    | 695/1273 [25:39<22:08,  2.30s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  55%|█████▍    | 696/1273 [25:41<21:33,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  55%|█████▍    | 697/1273 [25:43<20:52,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  55%|█████▍    | 698/1273 [25:45<20:24,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  55%|█████▍    | 699/1273 [25:47<20:19,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  55%|█████▍    | 700/1273 [25:49<21:33,  2.26s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  55%|█████▌    | 701/1273 [25:52<21:59,  2.31s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  55%|█████▌    | 702/1273 [25:54<20:54,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  55%|█████▌    | 703/1273 [25:56<20:31,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  55%|█████▌    | 704/1273 [25:58<19:59,  2.11s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  55%|█████▌    | 705/1273 [26:00<19:32,  2.06s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  55%|█████▌    | 706/1273 [26:03<21:12,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  56%|█████▌    | 707/1273 [26:05<21:42,  2.30s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  56%|█████▌    | 708/1273 [26:07<20:45,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  56%|█████▌    | 709/1273 [26:09<20:43,  2.21s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  56%|█████▌    | 710/1273 [26:11<20:12,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  56%|█████▌    | 711/1273 [26:13<19:54,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  56%|█████▌    | 712/1273 [26:16<21:51,  2.34s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  56%|█████▌    | 713/1273 [26:18<21:10,  2.27s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  56%|█████▌    | 714/1273 [26:20<20:25,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  56%|█████▌    | 715/1273 [26:22<20:04,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  56%|█████▌    | 716/1273 [26:24<19:31,  2.10s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  56%|█████▋    | 717/1273 [26:26<19:26,  2.10s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  56%|█████▋    | 718/1273 [26:29<21:39,  2.34s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  56%|█████▋    | 719/1273 [26:31<20:46,  2.25s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  57%|█████▋    | 720/1273 [26:33<20:13,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  57%|█████▋    | 721/1273 [26:35<19:45,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  57%|█████▋    | 722/1273 [26:37<19:17,  2.10s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  57%|█████▋    | 723/1273 [26:40<19:29,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  57%|█████▋    | 724/1273 [26:42<20:55,  2.29s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  57%|█████▋    | 725/1273 [26:44<20:40,  2.26s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  57%|█████▋    | 726/1273 [26:46<20:00,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  57%|█████▋    | 727/1273 [26:49<19:44,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  57%|█████▋    | 728/1273 [26:51<19:32,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  57%|█████▋    | 729/1273 [26:53<20:26,  2.26s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  57%|█████▋    | 730/1273 [26:56<21:01,  2.32s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  57%|█████▋    | 731/1273 [26:58<20:20,  2.25s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  58%|█████▊    | 732/1273 [27:00<19:30,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  58%|█████▊    | 733/1273 [27:02<19:26,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  58%|█████▊    | 734/1273 [27:04<19:10,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  58%|█████▊    | 735/1273 [27:07<20:20,  2.27s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  58%|█████▊    | 736/1273 [27:09<20:27,  2.29s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  58%|█████▊    | 737/1273 [27:11<19:58,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  58%|█████▊    | 738/1273 [27:13<19:14,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  58%|█████▊    | 739/1273 [27:15<18:52,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  58%|█████▊    | 740/1273 [27:17<18:55,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  58%|█████▊    | 741/1273 [27:20<20:26,  2.31s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  58%|█████▊    | 742/1273 [27:22<19:45,  2.23s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  58%|█████▊    | 743/1273 [27:24<19:02,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  58%|█████▊    | 744/1273 [27:26<18:41,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  59%|█████▊    | 745/1273 [27:28<18:31,  2.11s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  59%|█████▊    | 746/1273 [27:30<18:47,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  59%|█████▊    | 747/1273 [27:33<20:27,  2.33s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  59%|█████▉    | 748/1273 [27:35<19:50,  2.27s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  59%|█████▉    | 749/1273 [27:37<19:00,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  59%|█████▉    | 750/1273 [27:39<18:31,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  59%|█████▉    | 751/1273 [27:41<18:15,  2.10s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  59%|█████▉    | 752/1273 [27:43<18:27,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  59%|█████▉    | 753/1273 [27:46<19:59,  2.31s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  59%|█████▉    | 754/1273 [27:48<19:13,  2.22s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  59%|█████▉    | 755/1273 [27:50<18:51,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  59%|█████▉    | 756/1273 [27:52<18:36,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  59%|█████▉    | 757/1273 [27:54<18:15,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  60%|█████▉    | 758/1273 [27:57<18:33,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  60%|█████▉    | 759/1273 [27:59<19:52,  2.32s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  60%|█████▉    | 760/1273 [28:01<19:19,  2.26s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  60%|█████▉    | 761/1273 [28:03<18:54,  2.22s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  60%|█████▉    | 762/1273 [28:06<18:39,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  60%|█████▉    | 763/1273 [28:08<18:20,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  60%|██████    | 764/1273 [28:10<19:10,  2.26s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  60%|██████    | 765/1273 [28:13<19:41,  2.33s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  60%|██████    | 766/1273 [28:15<18:44,  2.22s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  60%|██████    | 767/1273 [28:17<18:03,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  60%|██████    | 768/1273 [28:19<17:51,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  60%|██████    | 769/1273 [28:21<17:35,  2.09s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  60%|██████    | 770/1273 [28:23<18:55,  2.26s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  61%|██████    | 771/1273 [28:26<19:12,  2.30s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  61%|██████    | 772/1273 [28:28<18:44,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  61%|██████    | 773/1273 [28:30<18:06,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  61%|██████    | 774/1273 [28:32<17:45,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  61%|██████    | 775/1273 [28:34<17:27,  2.10s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  61%|██████    | 776/1273 [28:37<18:39,  2.25s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  61%|██████    | 777/1273 [28:39<18:50,  2.28s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  61%|██████    | 778/1273 [28:41<18:14,  2.21s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  61%|██████    | 779/1273 [28:43<17:45,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  61%|██████▏   | 780/1273 [28:45<17:13,  2.10s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  61%|██████▏   | 781/1273 [28:47<17:22,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  61%|██████▏   | 782/1273 [28:50<19:10,  2.34s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  62%|██████▏   | 783/1273 [28:52<18:30,  2.27s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  62%|██████▏   | 784/1273 [28:54<18:01,  2.21s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  62%|██████▏   | 785/1273 [28:56<17:30,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  62%|██████▏   | 786/1273 [28:58<17:12,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  62%|██████▏   | 787/1273 [29:00<17:31,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  62%|██████▏   | 788/1273 [29:03<18:31,  2.29s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  62%|██████▏   | 789/1273 [29:05<17:55,  2.22s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  62%|██████▏   | 790/1273 [29:07<17:30,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  62%|██████▏   | 791/1273 [29:09<17:11,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  62%|██████▏   | 792/1273 [29:11<16:56,  2.11s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  62%|██████▏   | 793/1273 [29:14<17:51,  2.23s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  62%|██████▏   | 794/1273 [29:16<18:31,  2.32s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  62%|██████▏   | 795/1273 [29:18<17:49,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  63%|██████▎   | 796/1273 [29:20<17:13,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  63%|██████▎   | 797/1273 [29:22<16:53,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  63%|██████▎   | 798/1273 [29:24<16:31,  2.09s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  63%|██████▎   | 799/1273 [29:27<17:25,  2.21s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  63%|██████▎   | 800/1273 [29:29<17:47,  2.26s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  63%|██████▎   | 801/1273 [29:31<17:13,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  63%|██████▎   | 802/1273 [29:33<16:44,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  63%|██████▎   | 803/1273 [29:35<16:33,  2.11s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  63%|██████▎   | 804/1273 [29:37<16:15,  2.08s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  63%|██████▎   | 805/1273 [29:40<17:10,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  63%|██████▎   | 806/1273 [29:42<17:03,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  63%|██████▎   | 807/1273 [29:44<16:31,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  63%|██████▎   | 808/1273 [29:46<16:24,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  64%|██████▎   | 809/1273 [29:48<16:28,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  64%|██████▎   | 810/1273 [29:50<16:24,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  64%|██████▎   | 811/1273 [29:53<18:04,  2.35s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  64%|██████▍   | 812/1273 [29:55<17:39,  2.30s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  64%|██████▍   | 813/1273 [29:57<16:58,  2.21s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  64%|██████▍   | 814/1273 [29:59<16:29,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  64%|██████▍   | 815/1273 [30:02<16:21,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  64%|██████▍   | 816/1273 [30:04<16:41,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  64%|██████▍   | 817/1273 [30:07<17:44,  2.34s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  64%|██████▍   | 818/1273 [30:09<16:58,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  64%|██████▍   | 819/1273 [30:11<16:33,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  64%|██████▍   | 820/1273 [30:13<15:58,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  64%|██████▍   | 821/1273 [30:15<15:53,  2.11s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  65%|██████▍   | 822/1273 [30:17<16:29,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  65%|██████▍   | 823/1273 [30:20<17:33,  2.34s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  65%|██████▍   | 824/1273 [30:22<16:49,  2.25s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  65%|██████▍   | 825/1273 [30:24<16:33,  2.22s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  65%|██████▍   | 826/1273 [30:26<16:08,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  65%|██████▍   | 827/1273 [30:28<15:45,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  65%|██████▌   | 828/1273 [30:31<17:05,  2.31s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  65%|██████▌   | 829/1273 [30:33<16:52,  2.28s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  65%|██████▌   | 830/1273 [30:35<16:20,  2.21s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  65%|██████▌   | 831/1273 [30:37<15:45,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  65%|██████▌   | 832/1273 [30:39<15:38,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  65%|██████▌   | 833/1273 [30:41<15:32,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  66%|██████▌   | 834/1273 [30:44<17:12,  2.35s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  66%|██████▌   | 835/1273 [30:46<16:44,  2.29s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  66%|██████▌   | 836/1273 [30:48<15:56,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  66%|██████▌   | 837/1273 [30:50<15:39,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  66%|██████▌   | 838/1273 [30:52<15:35,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  66%|██████▌   | 839/1273 [30:54<15:20,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  66%|██████▌   | 840/1273 [30:57<16:46,  2.32s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  66%|██████▌   | 841/1273 [30:59<16:02,  2.23s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  66%|██████▌   | 842/1273 [31:01<15:27,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  66%|██████▌   | 843/1273 [31:03<15:14,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  66%|██████▋   | 844/1273 [31:05<15:13,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  66%|██████▋   | 845/1273 [31:08<15:17,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  66%|██████▋   | 846/1273 [31:10<16:39,  2.34s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  67%|██████▋   | 847/1273 [31:12<15:59,  2.25s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  67%|██████▋   | 848/1273 [31:14<15:29,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  67%|██████▋   | 849/1273 [31:16<14:59,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  67%|██████▋   | 850/1273 [31:18<14:37,  2.07s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  67%|██████▋   | 851/1273 [31:21<14:53,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  67%|██████▋   | 852/1273 [31:23<16:13,  2.31s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  67%|██████▋   | 853/1273 [31:26<15:55,  2.27s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  67%|██████▋   | 854/1273 [31:28<15:30,  2.22s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  67%|██████▋   | 855/1273 [31:30<14:54,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  67%|██████▋   | 856/1273 [31:32<14:52,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  67%|██████▋   | 857/1273 [31:34<15:24,  2.22s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  67%|██████▋   | 858/1273 [31:37<15:46,  2.28s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  67%|██████▋   | 859/1273 [31:39<15:15,  2.21s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  68%|██████▊   | 860/1273 [31:41<14:48,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  68%|██████▊   | 861/1273 [31:43<14:20,  2.09s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  68%|██████▊   | 862/1273 [31:45<14:07,  2.06s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  68%|██████▊   | 863/1273 [31:47<14:53,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  68%|██████▊   | 864/1273 [31:49<15:23,  2.26s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  68%|██████▊   | 865/1273 [31:51<14:49,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  68%|██████▊   | 866/1273 [31:54<14:44,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  68%|██████▊   | 867/1273 [31:56<14:21,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  68%|██████▊   | 868/1273 [31:58<14:12,  2.11s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  68%|██████▊   | 869/1273 [32:00<15:19,  2.28s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  68%|██████▊   | 870/1273 [32:03<15:03,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  68%|██████▊   | 871/1273 [32:05<14:39,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  68%|██████▊   | 872/1273 [32:07<14:27,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  69%|██████▊   | 873/1273 [32:09<14:23,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  69%|██████▊   | 874/1273 [32:11<14:26,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  69%|██████▊   | 875/1273 [32:14<15:40,  2.36s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  69%|██████▉   | 876/1273 [32:16<14:50,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  69%|██████▉   | 877/1273 [32:18<14:20,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  69%|██████▉   | 878/1273 [32:20<14:17,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  69%|██████▉   | 879/1273 [32:22<13:59,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  69%|██████▉   | 880/1273 [32:24<14:29,  2.21s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  69%|██████▉   | 881/1273 [32:27<15:22,  2.35s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  69%|██████▉   | 882/1273 [32:29<14:52,  2.28s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  69%|██████▉   | 883/1273 [32:31<14:33,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  69%|██████▉   | 884/1273 [32:33<14:01,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  70%|██████▉   | 885/1273 [32:35<13:47,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  70%|██████▉   | 886/1273 [32:38<14:25,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  70%|██████▉   | 887/1273 [32:40<14:47,  2.30s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  70%|██████▉   | 888/1273 [32:42<14:12,  2.21s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  70%|██████▉   | 889/1273 [32:44<13:49,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  70%|██████▉   | 890/1273 [32:46<13:35,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  70%|██████▉   | 891/1273 [32:48<13:23,  2.10s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  70%|███████   | 892/1273 [32:51<14:48,  2.33s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  70%|███████   | 893/1273 [32:54<14:44,  2.33s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  70%|███████   | 894/1273 [32:56<14:22,  2.27s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  70%|███████   | 895/1273 [32:58<13:46,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  70%|███████   | 896/1273 [33:00<13:36,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  70%|███████   | 897/1273 [33:02<13:23,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  71%|███████   | 898/1273 [33:05<14:39,  2.35s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  71%|███████   | 899/1273 [33:07<14:15,  2.29s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  71%|███████   | 900/1273 [33:09<14:02,  2.26s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  71%|███████   | 901/1273 [33:11<13:44,  2.22s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  71%|███████   | 902/1273 [33:13<13:29,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  71%|███████   | 903/1273 [33:16<13:49,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  71%|███████   | 904/1273 [33:18<14:26,  2.35s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  71%|███████   | 905/1273 [33:20<13:55,  2.27s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  71%|███████   | 906/1273 [33:23<13:39,  2.23s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  71%|███████   | 907/1273 [33:25<13:18,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  71%|███████▏  | 908/1273 [33:27<13:09,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  71%|███████▏  | 909/1273 [33:29<14:08,  2.33s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  71%|███████▏  | 910/1273 [33:32<13:38,  2.25s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  72%|███████▏  | 911/1273 [33:34<13:14,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  72%|███████▏  | 912/1273 [33:36<12:50,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  72%|███████▏  | 913/1273 [33:38<12:44,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  72%|███████▏  | 914/1273 [33:40<12:37,  2.11s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  72%|███████▏  | 915/1273 [33:43<13:41,  2.29s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  72%|███████▏  | 916/1273 [33:45<13:17,  2.23s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  72%|███████▏  | 917/1273 [33:47<12:51,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  72%|███████▏  | 918/1273 [33:49<12:33,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  72%|███████▏  | 919/1273 [33:51<12:20,  2.09s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  72%|███████▏  | 920/1273 [33:53<12:22,  2.10s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  72%|███████▏  | 921/1273 [33:56<13:24,  2.29s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  72%|███████▏  | 922/1273 [33:58<13:00,  2.22s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  73%|███████▎  | 923/1273 [34:00<12:45,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  73%|███████▎  | 924/1273 [34:02<12:33,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  73%|███████▎  | 925/1273 [34:04<12:25,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  73%|███████▎  | 926/1273 [34:06<12:33,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  73%|███████▎  | 927/1273 [34:09<13:18,  2.31s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  73%|███████▎  | 928/1273 [34:11<12:58,  2.26s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  73%|███████▎  | 929/1273 [34:13<12:38,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  73%|███████▎  | 930/1273 [34:15<12:18,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  73%|███████▎  | 931/1273 [34:17<12:10,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  73%|███████▎  | 932/1273 [34:20<13:05,  2.30s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  73%|███████▎  | 933/1273 [34:22<13:04,  2.31s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  73%|███████▎  | 934/1273 [34:24<12:41,  2.25s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  73%|███████▎  | 935/1273 [34:26<12:18,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  74%|███████▎  | 936/1273 [34:28<12:02,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  74%|███████▎  | 937/1273 [34:30<11:41,  2.09s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  74%|███████▎  | 938/1273 [34:33<12:57,  2.32s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  74%|███████▍  | 939/1273 [34:35<12:18,  2.21s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  74%|███████▍  | 940/1273 [34:37<11:57,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  74%|███████▍  | 941/1273 [34:39<11:39,  2.11s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  74%|███████▍  | 942/1273 [34:41<11:21,  2.06s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  74%|███████▍  | 943/1273 [34:43<11:29,  2.09s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  74%|███████▍  | 944/1273 [34:46<12:42,  2.32s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  74%|███████▍  | 945/1273 [34:48<12:30,  2.29s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  74%|███████▍  | 946/1273 [34:50<12:04,  2.22s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  74%|███████▍  | 947/1273 [34:52<11:52,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  74%|███████▍  | 948/1273 [34:54<11:33,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  75%|███████▍  | 949/1273 [34:57<11:33,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  75%|███████▍  | 950/1273 [34:59<12:10,  2.26s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  75%|███████▍  | 951/1273 [35:01<11:49,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  75%|███████▍  | 952/1273 [35:03<11:28,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  75%|███████▍  | 953/1273 [35:05<11:14,  2.11s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  75%|███████▍  | 954/1273 [35:07<11:09,  2.10s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  75%|███████▌  | 955/1273 [35:10<11:49,  2.23s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  75%|███████▌  | 956/1273 [35:12<12:00,  2.27s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  75%|███████▌  | 957/1273 [35:14<11:36,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  75%|███████▌  | 958/1273 [35:16<11:15,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  75%|███████▌  | 959/1273 [35:18<11:03,  2.11s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  75%|███████▌  | 960/1273 [35:20<10:52,  2.08s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  75%|███████▌  | 961/1273 [35:23<11:43,  2.25s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  76%|███████▌  | 962/1273 [35:25<11:37,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  76%|███████▌  | 963/1273 [35:27<11:19,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  76%|███████▌  | 964/1273 [35:29<10:58,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  76%|███████▌  | 965/1273 [35:31<10:45,  2.09s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  76%|███████▌  | 966/1273 [35:33<10:40,  2.08s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  76%|███████▌  | 967/1273 [35:36<11:35,  2.27s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  76%|███████▌  | 968/1273 [35:38<11:13,  2.21s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  76%|███████▌  | 969/1273 [35:40<11:01,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  76%|███████▌  | 970/1273 [35:42<10:45,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  76%|███████▋  | 971/1273 [35:44<10:39,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  76%|███████▋  | 972/1273 [35:47<10:48,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  76%|███████▋  | 973/1273 [35:49<11:17,  2.26s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  77%|███████▋  | 974/1273 [35:51<10:58,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  77%|███████▋  | 975/1273 [35:53<10:39,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  77%|███████▋  | 976/1273 [35:55<10:18,  2.08s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  77%|███████▋  | 977/1273 [35:57<10:13,  2.07s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  77%|███████▋  | 978/1273 [35:59<10:33,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  77%|███████▋  | 979/1273 [36:02<11:10,  2.28s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  77%|███████▋  | 980/1273 [36:04<10:45,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  77%|███████▋  | 981/1273 [36:06<10:37,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  77%|███████▋  | 982/1273 [36:08<10:26,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  77%|███████▋  | 983/1273 [36:10<10:13,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  77%|███████▋  | 984/1273 [36:13<11:00,  2.28s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  77%|███████▋  | 985/1273 [36:15<10:41,  2.23s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  77%|███████▋  | 986/1273 [36:17<10:31,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  78%|███████▊  | 987/1273 [36:19<10:13,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  78%|███████▊  | 988/1273 [36:21<09:59,  2.10s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  78%|███████▊  | 989/1273 [36:23<09:51,  2.08s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  78%|███████▊  | 990/1273 [36:26<10:56,  2.32s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  78%|███████▊  | 991/1273 [36:28<10:39,  2.27s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  78%|███████▊  | 992/1273 [36:30<10:16,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  78%|███████▊  | 993/1273 [36:32<09:59,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  78%|███████▊  | 994/1273 [36:34<09:51,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  78%|███████▊  | 995/1273 [36:37<09:58,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  78%|███████▊  | 996/1273 [36:39<10:39,  2.31s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  78%|███████▊  | 997/1273 [36:41<10:15,  2.23s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  78%|███████▊  | 998/1273 [36:43<09:49,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  78%|███████▊  | 999/1273 [36:45<09:38,  2.11s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  79%|███████▊  | 1000/1273 [36:47<09:29,  2.09s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  79%|███████▊  | 1001/1273 [36:50<09:46,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  79%|███████▊  | 1002/1273 [36:52<10:17,  2.28s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  79%|███████▉  | 1003/1273 [36:54<09:54,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  79%|███████▉  | 1004/1273 [36:56<09:38,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  79%|███████▉  | 1005/1273 [36:58<09:25,  2.11s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  79%|███████▉  | 1006/1273 [37:00<09:24,  2.11s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  79%|███████▉  | 1007/1273 [37:03<09:54,  2.23s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  79%|███████▉  | 1008/1273 [37:05<10:08,  2.30s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  79%|███████▉  | 1009/1273 [37:08<09:51,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  79%|███████▉  | 1010/1273 [37:10<09:39,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  79%|███████▉  | 1011/1273 [37:12<09:34,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  79%|███████▉  | 1012/1273 [37:14<09:23,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  80%|███████▉  | 1013/1273 [37:17<10:19,  2.38s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  80%|███████▉  | 1014/1273 [37:19<10:03,  2.33s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  80%|███████▉  | 1015/1273 [37:21<09:36,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  80%|███████▉  | 1016/1273 [37:23<09:24,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  80%|███████▉  | 1017/1273 [37:25<09:09,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  80%|███████▉  | 1018/1273 [37:27<09:17,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  80%|████████  | 1019/1273 [37:30<10:00,  2.36s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  80%|████████  | 1020/1273 [37:32<09:32,  2.26s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  80%|████████  | 1021/1273 [37:34<09:18,  2.22s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  80%|████████  | 1022/1273 [37:36<08:57,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  80%|████████  | 1023/1273 [37:38<08:55,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  80%|████████  | 1024/1273 [37:41<09:27,  2.28s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  81%|████████  | 1025/1273 [37:44<09:38,  2.33s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  81%|████████  | 1026/1273 [37:46<09:16,  2.25s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  81%|████████  | 1027/1273 [37:48<08:52,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  81%|████████  | 1028/1273 [37:50<08:50,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  81%|████████  | 1029/1273 [37:52<08:40,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  81%|████████  | 1030/1273 [37:54<09:06,  2.25s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  81%|████████  | 1031/1273 [37:57<09:14,  2.29s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  81%|████████  | 1032/1273 [37:59<08:56,  2.23s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  81%|████████  | 1033/1273 [38:01<08:40,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  81%|████████  | 1034/1273 [38:03<08:23,  2.10s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  81%|████████▏ | 1035/1273 [38:05<08:17,  2.09s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  81%|████████▏ | 1036/1273 [38:08<08:59,  2.27s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  81%|████████▏ | 1037/1273 [38:09<08:33,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  82%|████████▏ | 1038/1273 [38:11<08:18,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  82%|████████▏ | 1039/1273 [38:14<08:15,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  82%|████████▏ | 1040/1273 [38:16<08:10,  2.10s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  82%|████████▏ | 1041/1273 [38:18<08:19,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  82%|████████▏ | 1042/1273 [38:21<08:58,  2.33s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  82%|████████▏ | 1043/1273 [38:23<08:36,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  82%|████████▏ | 1044/1273 [38:25<08:18,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  82%|████████▏ | 1045/1273 [38:27<08:07,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  82%|████████▏ | 1046/1273 [38:29<08:00,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  82%|████████▏ | 1047/1273 [38:31<08:06,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  82%|████████▏ | 1048/1273 [38:34<08:48,  2.35s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  82%|████████▏ | 1049/1273 [38:36<08:22,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  82%|████████▏ | 1050/1273 [38:38<08:07,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  83%|████████▎ | 1051/1273 [38:40<07:52,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  83%|████████▎ | 1052/1273 [38:42<07:43,  2.10s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  83%|████████▎ | 1053/1273 [38:44<08:04,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  83%|████████▎ | 1054/1273 [38:47<08:31,  2.34s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  83%|████████▎ | 1055/1273 [38:49<08:13,  2.26s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  83%|████████▎ | 1056/1273 [38:51<07:50,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  83%|████████▎ | 1057/1273 [38:53<07:40,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  83%|████████▎ | 1058/1273 [38:55<07:28,  2.08s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  83%|████████▎ | 1059/1273 [38:58<07:53,  2.21s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  83%|████████▎ | 1060/1273 [39:00<08:08,  2.30s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  83%|████████▎ | 1061/1273 [39:02<07:47,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  83%|████████▎ | 1062/1273 [39:04<07:29,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  84%|████████▎ | 1063/1273 [39:06<07:20,  2.10s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  84%|████████▎ | 1064/1273 [39:08<07:10,  2.06s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  84%|████████▎ | 1065/1273 [39:10<07:28,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  84%|████████▎ | 1066/1273 [39:13<07:46,  2.26s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  84%|████████▍ | 1067/1273 [39:15<07:35,  2.21s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  84%|████████▍ | 1068/1273 [39:17<07:18,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  84%|████████▍ | 1069/1273 [39:19<07:09,  2.11s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  84%|████████▍ | 1070/1273 [39:21<07:04,  2.09s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  84%|████████▍ | 1071/1273 [39:24<07:25,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  84%|████████▍ | 1072/1273 [39:26<07:33,  2.26s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  84%|████████▍ | 1073/1273 [39:28<07:20,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  84%|████████▍ | 1074/1273 [39:30<07:07,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  84%|████████▍ | 1075/1273 [39:32<06:57,  2.11s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  85%|████████▍ | 1076/1273 [39:34<06:47,  2.07s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  85%|████████▍ | 1077/1273 [39:37<07:31,  2.30s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  85%|████████▍ | 1078/1273 [39:39<07:20,  2.26s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  85%|████████▍ | 1079/1273 [39:41<07:03,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  85%|████████▍ | 1080/1273 [39:43<06:55,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  85%|████████▍ | 1081/1273 [39:45<06:49,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  85%|████████▍ | 1082/1273 [39:47<06:51,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  85%|████████▌ | 1083/1273 [39:50<07:23,  2.33s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  85%|████████▌ | 1084/1273 [39:52<07:07,  2.26s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  85%|████████▌ | 1085/1273 [39:54<06:50,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  85%|████████▌ | 1086/1273 [39:56<06:37,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  85%|████████▌ | 1087/1273 [39:58<06:37,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  85%|████████▌ | 1088/1273 [40:01<06:40,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  86%|████████▌ | 1089/1273 [40:03<07:02,  2.30s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  86%|████████▌ | 1090/1273 [40:05<06:46,  2.22s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  86%|████████▌ | 1091/1273 [40:07<06:32,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  86%|████████▌ | 1092/1273 [40:09<06:23,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  86%|████████▌ | 1093/1273 [40:11<06:20,  2.11s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  86%|████████▌ | 1094/1273 [40:14<06:47,  2.28s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  86%|████████▌ | 1095/1273 [40:16<06:49,  2.30s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  86%|████████▌ | 1096/1273 [40:18<06:33,  2.22s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  86%|████████▌ | 1097/1273 [40:21<06:22,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  86%|████████▋ | 1098/1273 [40:23<06:16,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  86%|████████▋ | 1099/1273 [40:25<06:09,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  86%|████████▋ | 1100/1273 [40:27<06:36,  2.29s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  86%|████████▋ | 1101/1273 [40:30<06:36,  2.30s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  87%|████████▋ | 1102/1273 [40:32<06:21,  2.23s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  87%|████████▋ | 1103/1273 [40:34<06:13,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  87%|████████▋ | 1104/1273 [40:36<06:06,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  87%|████████▋ | 1105/1273 [40:38<06:00,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  87%|████████▋ | 1106/1273 [40:41<06:31,  2.34s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  87%|████████▋ | 1107/1273 [40:43<06:19,  2.29s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  87%|████████▋ | 1108/1273 [40:45<05:59,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  87%|████████▋ | 1109/1273 [40:47<05:49,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  87%|████████▋ | 1110/1273 [40:49<05:47,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  87%|████████▋ | 1111/1273 [40:51<05:39,  2.10s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  87%|████████▋ | 1112/1273 [40:54<06:23,  2.38s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  87%|████████▋ | 1113/1273 [40:56<06:06,  2.29s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  88%|████████▊ | 1114/1273 [40:58<05:50,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  88%|████████▊ | 1115/1273 [41:00<05:45,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  88%|████████▊ | 1116/1273 [41:02<05:33,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  88%|████████▊ | 1117/1273 [41:04<05:27,  2.10s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  88%|████████▊ | 1118/1273 [41:07<05:55,  2.29s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  88%|████████▊ | 1119/1273 [41:09<05:38,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  88%|████████▊ | 1120/1273 [41:11<05:27,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  88%|████████▊ | 1121/1273 [41:13<05:20,  2.11s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  88%|████████▊ | 1122/1273 [41:15<05:21,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  88%|████████▊ | 1123/1273 [41:18<05:33,  2.23s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  88%|████████▊ | 1124/1273 [41:20<05:39,  2.28s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  88%|████████▊ | 1125/1273 [41:22<05:31,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  88%|████████▊ | 1126/1273 [41:24<05:23,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  89%|████████▊ | 1127/1273 [41:26<05:13,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  89%|████████▊ | 1128/1273 [41:28<05:04,  2.10s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  89%|████████▊ | 1129/1273 [41:31<05:35,  2.33s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  89%|████████▉ | 1130/1273 [41:34<05:34,  2.34s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  89%|████████▉ | 1131/1273 [41:36<05:21,  2.26s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  89%|████████▉ | 1132/1273 [41:38<05:07,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  89%|████████▉ | 1133/1273 [41:40<05:03,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  89%|████████▉ | 1134/1273 [41:42<04:53,  2.11s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  89%|████████▉ | 1135/1273 [41:45<05:19,  2.32s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  89%|████████▉ | 1136/1273 [41:47<05:06,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  89%|████████▉ | 1137/1273 [41:49<04:56,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  89%|████████▉ | 1138/1273 [41:51<04:45,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  89%|████████▉ | 1139/1273 [41:53<04:42,  2.11s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  90%|████████▉ | 1140/1273 [41:55<04:53,  2.21s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  90%|████████▉ | 1141/1273 [41:58<05:04,  2.30s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  90%|████████▉ | 1142/1273 [42:00<04:52,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  90%|████████▉ | 1143/1273 [42:02<04:42,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  90%|████████▉ | 1144/1273 [42:04<04:35,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  90%|████████▉ | 1145/1273 [42:06<04:28,  2.10s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  90%|█████████ | 1146/1273 [42:09<04:46,  2.25s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  90%|█████████ | 1147/1273 [42:11<04:47,  2.28s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  90%|█████████ | 1148/1273 [42:13<04:35,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  90%|█████████ | 1149/1273 [42:15<04:24,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  90%|█████████ | 1150/1273 [42:17<04:20,  2.11s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  90%|█████████ | 1151/1273 [42:19<04:14,  2.09s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  90%|█████████ | 1152/1273 [42:22<04:27,  2.21s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  91%|█████████ | 1153/1273 [42:24<04:28,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  91%|█████████ | 1154/1273 [42:26<04:23,  2.22s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  91%|█████████ | 1155/1273 [42:28<04:12,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  91%|█████████ | 1156/1273 [42:30<04:06,  2.11s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  91%|█████████ | 1157/1273 [42:32<04:05,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  91%|█████████ | 1158/1273 [42:35<04:23,  2.29s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  91%|█████████ | 1159/1273 [42:37<04:22,  2.30s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  91%|█████████ | 1160/1273 [42:39<04:13,  2.25s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  91%|█████████ | 1161/1273 [42:41<04:03,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  91%|█████████▏| 1162/1273 [42:43<03:56,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  91%|█████████▏| 1163/1273 [42:45<03:50,  2.10s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  91%|█████████▏| 1164/1273 [42:48<04:05,  2.25s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  92%|█████████▏| 1165/1273 [42:50<03:55,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  92%|█████████▏| 1166/1273 [42:52<03:55,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  92%|█████████▏| 1167/1273 [42:54<03:46,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  92%|█████████▏| 1168/1273 [42:56<03:40,  2.10s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  92%|█████████▏| 1169/1273 [42:58<03:43,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  92%|█████████▏| 1170/1273 [43:01<03:52,  2.25s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  92%|█████████▏| 1171/1273 [43:03<03:43,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  92%|█████████▏| 1172/1273 [43:05<03:40,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  92%|█████████▏| 1173/1273 [43:07<03:34,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  92%|█████████▏| 1174/1273 [43:09<03:32,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  92%|█████████▏| 1175/1273 [43:12<03:47,  2.32s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  92%|█████████▏| 1176/1273 [43:14<03:42,  2.29s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  92%|█████████▏| 1177/1273 [43:16<03:35,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  93%|█████████▎| 1178/1273 [43:18<03:27,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  93%|█████████▎| 1179/1273 [43:20<03:18,  2.11s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  93%|█████████▎| 1180/1273 [43:23<03:16,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  93%|█████████▎| 1181/1273 [43:25<03:35,  2.34s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  93%|█████████▎| 1182/1273 [43:28<03:28,  2.29s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  93%|█████████▎| 1183/1273 [43:30<03:18,  2.21s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  93%|█████████▎| 1184/1273 [43:32<03:13,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  93%|█████████▎| 1185/1273 [43:34<03:06,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  93%|█████████▎| 1186/1273 [43:36<03:12,  2.21s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  93%|█████████▎| 1187/1273 [43:39<03:19,  2.31s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  93%|█████████▎| 1188/1273 [43:41<03:09,  2.23s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  93%|█████████▎| 1189/1273 [43:43<03:02,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  93%|█████████▎| 1190/1273 [43:45<02:58,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  94%|█████████▎| 1191/1273 [43:47<02:52,  2.10s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  94%|█████████▎| 1192/1273 [43:49<02:55,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  94%|█████████▎| 1193/1273 [43:52<03:03,  2.29s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  94%|█████████▍| 1194/1273 [43:54<02:55,  2.22s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  94%|█████████▍| 1195/1273 [43:56<02:49,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  94%|█████████▍| 1196/1273 [43:58<02:44,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  94%|█████████▍| 1197/1273 [44:00<02:41,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  94%|█████████▍| 1198/1273 [44:02<02:44,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  94%|█████████▍| 1199/1273 [44:05<02:52,  2.33s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  94%|█████████▍| 1200/1273 [44:07<02:45,  2.26s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  94%|█████████▍| 1201/1273 [44:09<02:40,  2.22s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  94%|█████████▍| 1202/1273 [44:11<02:33,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  95%|█████████▍| 1203/1273 [44:13<02:28,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  95%|█████████▍| 1204/1273 [44:16<02:35,  2.26s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  95%|█████████▍| 1205/1273 [44:18<02:37,  2.32s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  95%|█████████▍| 1206/1273 [44:20<02:28,  2.22s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  95%|█████████▍| 1207/1273 [44:22<02:23,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  95%|█████████▍| 1208/1273 [44:24<02:19,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  95%|█████████▍| 1209/1273 [44:26<02:15,  2.11s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  95%|█████████▌| 1210/1273 [44:29<02:24,  2.30s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  95%|█████████▌| 1211/1273 [44:31<02:18,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  95%|█████████▌| 1212/1273 [44:33<02:12,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  95%|█████████▌| 1213/1273 [44:35<02:07,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  95%|█████████▌| 1214/1273 [44:37<02:03,  2.09s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  95%|█████████▌| 1215/1273 [44:39<02:00,  2.07s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  96%|█████████▌| 1216/1273 [44:42<02:10,  2.28s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  96%|█████████▌| 1217/1273 [44:44<02:06,  2.25s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  96%|█████████▌| 1218/1273 [44:46<01:59,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  96%|█████████▌| 1219/1273 [44:48<01:54,  2.11s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  96%|█████████▌| 1220/1273 [44:50<01:50,  2.08s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  96%|█████████▌| 1221/1273 [44:52<01:49,  2.10s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  96%|█████████▌| 1222/1273 [44:55<01:57,  2.31s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  96%|█████████▌| 1223/1273 [44:57<01:50,  2.21s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  96%|█████████▌| 1224/1273 [44:59<01:45,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  96%|█████████▌| 1225/1273 [45:01<01:42,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  96%|█████████▋| 1226/1273 [45:03<01:39,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  96%|█████████▋| 1227/1273 [45:06<01:37,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  96%|█████████▋| 1228/1273 [45:08<01:43,  2.29s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  97%|█████████▋| 1229/1273 [45:10<01:37,  2.21s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  97%|█████████▋| 1230/1273 [45:12<01:34,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  97%|█████████▋| 1231/1273 [45:14<01:30,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  97%|█████████▋| 1232/1273 [45:16<01:26,  2.11s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  97%|█████████▋| 1233/1273 [45:19<01:27,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  97%|█████████▋| 1234/1273 [45:21<01:29,  2.30s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  97%|█████████▋| 1235/1273 [45:24<01:26,  2.27s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  97%|█████████▋| 1236/1273 [45:26<01:21,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  97%|█████████▋| 1237/1273 [45:28<01:17,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  97%|█████████▋| 1238/1273 [45:30<01:14,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  97%|█████████▋| 1239/1273 [45:33<01:19,  2.33s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  97%|█████████▋| 1240/1273 [45:35<01:15,  2.29s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  97%|█████████▋| 1241/1273 [45:37<01:10,  2.22s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  98%|█████████▊| 1242/1273 [45:39<01:07,  2.18s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  98%|█████████▊| 1243/1273 [45:41<01:04,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  98%|█████████▊| 1244/1273 [45:43<01:02,  2.14s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  98%|█████████▊| 1245/1273 [45:46<01:04,  2.29s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  98%|█████████▊| 1246/1273 [45:48<01:00,  2.23s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  98%|█████████▊| 1247/1273 [45:50<00:56,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  98%|█████████▊| 1248/1273 [45:52<00:53,  2.13s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  98%|█████████▊| 1249/1273 [45:54<00:50,  2.10s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  98%|█████████▊| 1250/1273 [45:56<00:50,  2.19s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  98%|█████████▊| 1251/1273 [45:59<00:51,  2.32s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  98%|█████████▊| 1252/1273 [46:01<00:47,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  98%|█████████▊| 1253/1273 [46:03<00:43,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  99%|█████████▊| 1254/1273 [46:05<00:39,  2.10s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  99%|█████████▊| 1255/1273 [46:07<00:37,  2.10s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  99%|█████████▊| 1256/1273 [46:09<00:36,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  99%|█████████▊| 1257/1273 [46:12<00:36,  2.30s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  99%|█████████▉| 1258/1273 [46:14<00:33,  2.24s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  99%|█████████▉| 1259/1273 [46:16<00:30,  2.16s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  99%|█████████▉| 1260/1273 [46:18<00:27,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  99%|█████████▉| 1261/1273 [46:20<00:25,  2.10s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  99%|█████████▉| 1262/1273 [46:23<00:24,  2.21s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  99%|█████████▉| 1263/1273 [46:25<00:23,  2.36s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  99%|█████████▉| 1264/1273 [46:27<00:20,  2.30s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  99%|█████████▉| 1265/1273 [46:29<00:17,  2.21s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:  99%|█████████▉| 1266/1273 [46:32<00:15,  2.20s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions: 100%|█████████▉| 1267/1273 [46:34<00:12,  2.15s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions: 100%|█████████▉| 1268/1273 [46:36<00:11,  2.34s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions: 100%|█████████▉| 1269/1273 [46:39<00:09,  2.30s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions: 100%|█████████▉| 1270/1273 [46:41<00:06,  2.25s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions: 100%|█████████▉| 1271/1273 [46:43<00:04,  2.17s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions: 100%|█████████▉| 1272/1273 [46:45<00:02,  2.12s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions: 100%|██████████| 1273/1273 [46:47<00:00,  2.21s/item]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation results saved to final_evaluation_results.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "# Ensure model is in evaluation mode\n",
        "f_model.eval()\n",
        "\n",
        "# Prepare for predictions\n",
        "predictions = []\n",
        "true_labels = []  # To store true labels\n",
        "device = next(f_model.parameters()).device  # Ensure device compatibility\n",
        "\n",
        "# Iterate through the test dataset\n",
        "for item in tqdm(test_dataset_custom, desc=\"Generating predictions\", unit=\"item\"):\n",
        "    # Extract input tensors and move to the same device as the model\n",
        "    input_ids = item['input_ids'].unsqueeze(0).to(device)  # Add batch dimension\n",
        "    attention_mask = item['attention_mask'].unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "    # Generate outputs from the model\n",
        "    outputs = f_model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=40)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Append predictions\n",
        "    predictions.append(response)\n",
        "\n",
        "    # Append true label\n",
        "    true_labels.append(item['labels'].item())  # Extract the true label\n",
        "\n",
        "# Save predictions and true labels to a single CSV file\n",
        "output_df = pd.DataFrame({\n",
        "    \"Predicted Output\": predictions,\n",
        "    \"True Label\": true_labels\n",
        "})\n",
        "output_df.to_csv(\"final_evaluation_results2.csv\", index=False)\n",
        "\n",
        "print(\"Evaluation results saved to final_evaluation_results2.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "PqYkQV_zNlSg",
        "outputId": "f8f63585-88b1-4750-c532-6008cd4812ff"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions:   0%|          | 0/1273 [00:00<?, ?item/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   0%|          | 1/1273 [00:08<3:07:54,  8.86s/item]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Generating predictions:   0%|          | 1/1273 [00:14<4:57:09, 14.02s/item]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-e8165b53419f>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Generate outputs from the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2214\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2215\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2216\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2217\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1190\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1191\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    943\u001b[0m                 )\n\u001b[1;32m    944\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 945\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    946\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         hidden_states, self_attn_weights, present_key_value = self.self_attn(\n\u001b[0m\u001b[1;32m    677\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0mquery_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mkey_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mvalue_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0mquery_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1007\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1009\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_fp16_weights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(A, B, out, state, threshold, bias)\u001b[0m\n\u001b[1;32m    554\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mMatMul8bitLt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_setup_ctx_defined\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, A, B, out, bias, state)\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutliers\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSCB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m127.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0mCA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m             \u001b[0mCAt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m             \u001b[0msubA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "6NWjBHNMNvh7",
        "outputId": "603e99de-df82-4e91-c8c6-c3626d2544e1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    }
  ]
}
